#!/usr/bin/env python3
"""Seed content for papers group 2 (Reasoning + Spatial/3D)."""
from __future__ import annotations

import json
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).resolve().parent.parent))
from app.db import get_admin_client

PAPERS = [
    # ── #18  ECoT ──────────────────────────────────────────────────────
    {
        "number": 18,
        "summary": (
            "Embodied Chain-of-Thought (ECoT) augments robot action prediction by "
            "generating explicit textual reasoning traces before outputting actions. "
            "Built on top of RT-2, the model first produces a chain-of-thought describing "
            "the scene, the task plan, and the intended motion, then conditions its action "
            "tokens on that reasoning. This structured intermediate reasoning significantly "
            "improves generalization to novel objects and instructions."
        ),
        "key_learnings": (
            "- ECoT inserts an explicit chain-of-thought reasoning step between observation and action, forcing the model to articulate its plan before acting\n"
            "- The reasoning traces are generated as natural language tokens that precede the action tokens in the output sequence\n"
            "- Training data for reasoning traces is created by prompting a VLM to describe scenes and plans, then pairing these with robot demonstrations\n"
            "- ECoT improves generalization to unseen objects and instructions because the reasoning step grounds the model's semantic knowledge before action prediction\n"
            "- The approach demonstrates that making internal reasoning explicit (rather than implicit in hidden states) provides a useful inductive bias for robotic control"
        ),
        "reading_guide": (
            "Start with Section 1 (Introduction) to understand why implicit reasoning in VLAs is insufficient. "
            "Focus on Section 3 (Method) for how reasoning traces are generated and integrated into the action prediction pipeline. "
            "Pay close attention to how training data for the chain-of-thought is constructed — this is the key practical contribution. "
            "Review Section 4 (Experiments) for the generalization results, especially the ablation on reasoning trace components. "
            "Skim Section 2 (Related Work) unless you need background on chain-of-thought prompting in NLP."
        ),
        "questions": [
            {
                "question": "ECoT generates text reasoning traces before action tokens. What is the primary risk of this approach compared to models that reason implicitly in their hidden states?",
                "choices": [
                    "The text reasoning may be unfaithful to the model's actual internal reasoning, creating a false sense of interpretability",
                    "Text generation is always slower than direct action prediction, making real-time control impossible",
                    "Language-based reasoning cannot capture spatial relationships needed for manipulation",
                    "Chain-of-thought requires significantly more training data than standard VLA fine-tuning",
                ],
                "correct_index": 0,
                "explanation": "The fundamental risk is faithfulness: the generated text may be a post-hoc rationalization rather than a causal driver of the action. The model might learn to produce plausible-sounding reasoning that doesn't actually influence its action prediction, similar to known issues with chain-of-thought in LLMs. While the text is trained to precede actions, there's no guarantee the model conditions on it meaningfully versus using parallel pathways in its hidden states.",
                "why_it_matters": "This connects to a deep question in AI interpretability: does making a model verbalize its reasoning actually make it more interpretable or controllable? In robotics, if the reasoning trace says 'I will grasp from the left' but the model acts differently, the trace is misleading. Understanding this limitation is critical for deploying reasoning-augmented VLAs in safety-critical settings.",
            },
            {
                "question": "ECoT's training data for reasoning traces is generated by prompting a VLM to describe scenes and plans. What is the most significant limitation of this data generation strategy?",
                "choices": [
                    "VLMs cannot generate text descriptions of robotic scenes",
                    "The reasoning traces are bounded by the VLM's own understanding, potentially encoding biases and missing manipulation-specific reasoning that a human expert would provide",
                    "Generated traces are too verbose for real-time inference",
                    "VLM-generated traces cannot be paired with action labels because they use different tokenizers",
                ],
                "correct_index": 1,
                "explanation": "The quality of ECoT's reasoning is fundamentally limited by the teacher VLM's capabilities. If the VLM doesn't understand force-based reasoning, contact physics, or manipulation-specific affordances, those aspects will be absent from the training traces. This creates a ceiling effect where the robot's reasoning quality is bounded by a model trained on internet text, not physical interaction.",
                "why_it_matters": "This is a recurring pattern in robotics: using web-trained models to bootstrap capabilities they weren't designed for. The gap between internet-scale visual-language understanding and embodied physical reasoning is real. Understanding this ceiling helps you evaluate when VLM-distilled reasoning will help (semantic tasks) versus when it won't (tasks requiring physical intuition not captured in text).",
            },
            {
                "question": "Compared to CoT-VLA (#20), which uses visual reasoning chains, what fundamental trade-off does ECoT's text-based reasoning approach make?",
                "choices": [
                    "Text reasoning is faster to generate but less accurate",
                    "Text reasoning leverages LLM pre-training better but loses spatial precision that visual representations naturally encode",
                    "Text reasoning requires more GPU memory due to longer token sequences",
                    "Text reasoning works only for language-conditioned tasks while visual reasoning generalizes to any task specification",
                ],
                "correct_index": 1,
                "explanation": "Text-based reasoning (ECoT) can leverage the rich semantic and logical reasoning capabilities baked into LLM pre-training — planning, causal reasoning, commonsense knowledge. However, text is a lossy medium for spatial information: describing '3cm to the left of the red cup' in words is less precise and harder to ground than directly annotating an image. CoT-VLA's visual chains preserve spatial fidelity at the cost of weaker access to LLM-style abstract reasoning.",
                "why_it_matters": "This text-vs-visual reasoning trade-off is one of the central design decisions in modern VLA architectures. It determines what kinds of tasks the model excels at: text reasoning is better for tasks requiring semantic understanding and planning, while visual reasoning is better for tasks requiring precise spatial manipulation. Most real-world tasks need both, which is why hybrid approaches are an active research direction.",
            },
        ],
    },
    # ── #19  CoA-VLA ───────────────────────────────────────────────────
    {
        "number": 19,
        "summary": (
            "Chain-of-Affordance VLA (CoA-VLA) enhances Vision-Language-Action models by "
            "introducing an intermediate affordance reasoning step. Before predicting actions, "
            "the model generates visual-text affordance representations that identify what parts "
            "of the scene are manipulable and how. This affordance grounding bridges the gap "
            "between high-level language understanding and low-level motor control."
        ),
        "key_learnings": (
            "- CoA-VLA introduces affordance reasoning as an intermediate representation between language instructions and action outputs\n"
            "- Affordances encode what actions are possible on which objects — combining geometric, functional, and semantic information\n"
            "- The chain-of-affordance approach generates both visual (spatial) and textual (semantic) affordance cues before action prediction\n"
            "- By grounding reasoning in affordances rather than abstract plans, the model produces more physically plausible actions\n"
            "- CoA-VLA demonstrates that the choice of intermediate representation matters: affordances outperform generic chain-of-thought for manipulation tasks"
        ),
        "reading_guide": (
            "Begin with Section 1 to understand the motivation: why generic chain-of-thought is insufficient for manipulation. "
            "Focus heavily on Section 3 (Method) to understand how affordance representations are defined and generated. "
            "Pay attention to how visual and textual affordance cues are combined — this is the key architectural insight. "
            "In Section 4 (Experiments), focus on comparisons with ECoT and standard VLAs to understand when affordance reasoning helps most. "
            "The ablation study on different affordance components is essential for understanding which aspects drive performance."
        ),
        "questions": [
            {
                "question": "CoA-VLA uses affordance reasoning rather than generic chain-of-thought. Why are affordances a better intermediate representation for manipulation tasks specifically?",
                "choices": [
                    "Affordances are easier to train because they require less annotation",
                    "Affordances directly encode action possibilities grounded in object geometry and physics, while generic CoT may reason about semantically correct but physically infeasible actions",
                    "Affordances compress information more efficiently, allowing faster inference",
                    "Affordances are language-agnostic, enabling cross-lingual transfer",
                ],
                "correct_index": 1,
                "explanation": "Affordances are inherently about what actions are physically possible — they encode grasp points, contact surfaces, motion constraints. Generic chain-of-thought might produce a correct high-level plan ('pick up the mug') but fail to reason about the physical feasibility of specific grasps. Affordances bridge this gap by making the intermediate representation manipulation-aware rather than purely semantic.",
                "why_it_matters": "The choice of intermediate representation is arguably more important than the choice of backbone architecture. This question tests whether you understand why — in manipulation — reasoning about physical action possibilities (affordances) is fundamentally different from reasoning about abstract plans. This insight applies broadly: the best intermediate representation depends on the downstream task's requirements.",
            },
            {
                "question": "CoA-VLA combines both visual and textual affordance cues. What would you expect to happen if you ablated the visual component and kept only text-based affordance reasoning?",
                "choices": [
                    "Performance would improve because text is a more compact representation",
                    "No significant change because the text fully describes the affordances",
                    "Performance would degrade primarily on tasks requiring precise spatial localization of grasp points and contact regions",
                    "The model would fail entirely because text cannot represent affordances",
                ],
                "correct_index": 2,
                "explanation": "Text-only affordance reasoning can describe what to grasp and why, but struggles with precise where — the exact pixel location, orientation, and approach vector for a grasp. Visual affordance cues (like heatmaps or keypoint annotations) encode this spatial information natively. Tasks requiring precise positioning — thin object grasps, insertions, tool use — would be most affected by removing the visual component.",
                "why_it_matters": "This tests understanding of the complementary roles of visual and textual representations in robotics. Text excels at semantic and functional reasoning ('this is a handle, grasp it'), while vision excels at spatial precision ('grasp here at this angle'). Many VLA design decisions revolve around how to best combine these modalities, and understanding their individual strengths helps evaluate future architectures.",
            },
            {
                "question": "Compared to ECoT (#18) which uses free-form text reasoning, CoA-VLA's structured affordance representation makes a specific trade-off. What is it?",
                "choices": [
                    "CoA-VLA trades generality of reasoning for manipulation-specific grounding — it reasons better about physical interactions but may be less flexible for tasks requiring abstract planning or commonsense",
                    "CoA-VLA trades inference speed for accuracy",
                    "CoA-VLA trades model capacity for data efficiency",
                    "CoA-VLA trades visual quality for text quality",
                ],
                "correct_index": 0,
                "explanation": "By constraining the intermediate representation to affordances, CoA-VLA gains strong physical grounding for manipulation but loses the flexibility of free-form reasoning. ECoT can reason about arbitrary aspects of a task ('this looks fragile, I should be gentle'), while CoA-VLA's structured affordance format may not capture such open-ended reasoning. This is a classic bias-variance trade-off in the representation space.",
                "why_it_matters": "Structured vs. unstructured intermediate representations is a fundamental design choice that recurs throughout ML. More structure provides better inductive bias and data efficiency for in-distribution tasks, but reduces flexibility for out-of-distribution scenarios. Understanding this trade-off helps you predict when each approach will succeed or fail.",
            },
        ],
    },
    # ── #20  CoT-VLA ───────────────────────────────────────────────────
    {
        "number": 20,
        "summary": (
            "CoT-VLA introduces visual chain-of-thought reasoning for Vision-Language-Action "
            "models, replacing text-based reasoning chains with visual annotations such as "
            "attention maps and spatial markers overlaid on the input image. By reasoning in "
            "image space rather than text space, the model maintains spatial precision while "
            "still performing multi-step deliberation before action prediction."
        ),
        "key_learnings": (
            "- CoT-VLA performs chain-of-thought reasoning in visual space (attention maps, spatial annotations) rather than text space\n"
            "- Visual reasoning chains preserve the spatial structure of the scene, avoiding the lossy text-based description of positions and orientations\n"
            "- The model generates intermediate visual representations (e.g., annotated images with keypoints or highlighted regions) before predicting actions\n"
            "- This approach is more naturally grounded for manipulation tasks where spatial precision matters more than abstract reasoning\n"
            "- CoT-VLA demonstrates that the modality of reasoning (visual vs. text) significantly impacts which tasks benefit from chain-of-thought"
        ),
        "reading_guide": (
            "Start with Section 1 to understand the key insight: why text-based CoT loses spatial information critical for manipulation. "
            "Section 3 (Method) is the most important — focus on how visual reasoning chains are represented and generated. "
            "Pay special attention to what types of visual annotations are used (keypoints, heatmaps, bounding boxes) and how they feed into action prediction. "
            "In experiments, compare with ECoT (#18) results to understand when visual vs. text reasoning is preferable. "
            "The qualitative visualizations are unusually informative in this paper — examine them carefully."
        ),
        "questions": [
            {
                "question": "CoT-VLA reasons in image space using visual annotations. What fundamental advantage does this have over text-based CoT for manipulation, beyond just 'preserving spatial information'?",
                "choices": [
                    "Visual reasoning is faster to compute because images have fewer tokens than text",
                    "Visual reasoning chains can be directly supervised with human gaze data",
                    "Visual annotations create an implicit attention mechanism that biases the model's visual encoder toward task-relevant regions, improving feature extraction for action prediction",
                    "Visual reasoning eliminates the need for language understanding entirely",
                ],
                "correct_index": 2,
                "explanation": "Beyond spatial precision, visual annotations (highlighted regions, keypoints) act as a form of visual attention guidance. When the model generates annotations indicating 'focus on this region,' it effectively re-weights its own visual processing for the action prediction step. This is more than just information preservation — it's active modulation of the visual pipeline, something text-based reasoning cannot achieve because text doesn't directly interact with the visual feature extraction.",
                "why_it_matters": "This reveals a subtle but important point about multi-modal architectures: the modality in which you reason determines which processing pathways you can influence. Visual reasoning can modulate visual features; text reasoning can modulate language-conditioned planning. Understanding this helps you predict which reasoning modality will benefit which downstream task.",
            },
            {
                "question": "A key challenge for CoT-VLA is obtaining training supervision for visual reasoning chains. What makes this harder than obtaining supervision for text-based chains (as in ECoT)?",
                "choices": [
                    "Visual annotations require expensive 3D scanning equipment",
                    "Unlike text reasoning which can be generated by prompting existing LLMs/VLMs, there is no large pre-trained model that naturally produces manipulation-relevant visual reasoning annotations",
                    "Visual annotations take more storage space than text",
                    "Visual reasoning chains cannot be validated by humans",
                ],
                "correct_index": 1,
                "explanation": "ECoT can leverage existing VLMs to generate text reasoning traces at scale — just prompt the model to describe its plan. But for visual reasoning chains (keypoints, attention maps, spatial annotations), there's no equivalent generator. These annotations must be constructed through heuristics, human labeling, or task-specific algorithms, making the data pipeline significantly harder to scale.",
                "why_it_matters": "Data generation scalability is often the practical bottleneck that determines which research directions succeed. Text-based approaches benefit enormously from the LLM ecosystem's ability to generate training data. Visual reasoning approaches are more principled for manipulation but face a bootstrapping problem. This trade-off between theoretical elegance and practical scalability recurs throughout ML.",
            },
            {
                "question": "If you were designing a system that needed both precise spatial manipulation AND high-level task planning, how would you combine insights from CoT-VLA (visual reasoning) and ECoT (text reasoning)?",
                "choices": [
                    "Use only visual reasoning since it subsumes text reasoning",
                    "Alternate between visual and text reasoning at each timestep",
                    "Use text reasoning for high-level plan decomposition and goal specification, then visual reasoning for spatial grounding and precise action prediction within each sub-goal",
                    "Train two separate models and average their action predictions",
                ],
                "correct_index": 2,
                "explanation": "A hierarchical approach leverages each modality's strength: text reasoning excels at abstract planning, step decomposition, and commonsense ('first remove the lid, then pour') while visual reasoning excels at spatial precision ('grasp here, move to this exact position'). Using text for high-level planning and visual reasoning for low-level execution mirrors how humans combine verbal planning with spatial perception.",
                "why_it_matters": "This tests architectural design intuition. Real-world robotic systems need both abstract planning and precise execution. Understanding how to compose different reasoning modalities at different levels of abstraction is essential for building practical systems. This hierarchical composition principle applies broadly beyond robotics — it's a general pattern in complex AI system design.",
            },
        ],
    },
    # ── #22  MoTVLA ────────────────────────────────────────────────────
    {
        "number": 22,
        "summary": (
            "MoTVLA (Mixture of Thought VLA) unifies fast reactive control and slow deliberate "
            "reasoning within a single Vision-Language-Action model, inspired by Kahneman's "
            "dual-process theory (System 1 / System 2). The model dynamically routes between "
            "a fast pathway that produces immediate actions and a slow pathway that performs "
            "explicit reasoning before acting, depending on the complexity of the current situation."
        ),
        "key_learnings": (
            "- MoTVLA implements dual-process reasoning with a fast pathway (reactive, low-latency) and a slow pathway (deliberative, higher-latency) in one unified model\n"
            "- The routing mechanism decides when to invoke slow reasoning (novel/complex situations) vs. fast action (routine/familiar situations)\n"
            "- This mirrors human cognition: most actions are fast and automatic, but we switch to deliberate reasoning when faced with novel challenges\n"
            "- Unifying both pathways in one model avoids the overhead and error propagation of separate planning and execution modules\n"
            "- The approach addresses a key limitation of pure CoT methods: not every timestep requires expensive deliberation, and forcing it adds unnecessary latency"
        ),
        "reading_guide": (
            "Section 1 provides essential motivation — understand the dual-process theory framing and why constant deliberation is wasteful. "
            "Focus on Section 3 for the architecture: how the fast and slow pathways are implemented and how routing works. "
            "The routing mechanism design is critical — understand what signals trigger slow reasoning. "
            "In experiments, pay attention to the latency analysis alongside accuracy: the whole point is that MoTVLA matches slow-reasoning accuracy with much lower average latency. "
            "Compare with ECoT (#18) and CoT-VLA (#20) to understand where constant reasoning hurts."
        ),
        "questions": [
            {
                "question": "MoTVLA's routing mechanism must decide when to invoke slow deliberative reasoning. What is the most challenging aspect of designing this router?",
                "choices": [
                    "The router adds too many parameters to the model",
                    "Novelty and complexity are hard to quantify — the model must assess whether its fast pathway will fail BEFORE it actually fails, which requires a form of meta-cognition",
                    "The router requires a separate training dataset with complexity labels",
                    "Fast and slow pathways cannot share the same visual encoder",
                ],
                "correct_index": 1,
                "explanation": "The router must predict the fast pathway's reliability in the current situation — essentially performing uncertainty estimation or novelty detection. This is meta-cognitive: the model needs to know what it doesn't know. If the router is too conservative (always routes to slow), you lose the latency benefit. If too aggressive (always fast), you lose the accuracy benefit on hard cases. Calibrating this threshold is the core design challenge.",
                "why_it_matters": "This connects to a fundamental problem in ML: knowing when your model is likely to be wrong. Uncertainty estimation, out-of-distribution detection, and knowing when to 'think harder' are unsolved problems that appear across all of ML. MoTVLA's routing mechanism is a concrete instantiation of this challenge, and understanding it helps you evaluate any system that claims adaptive computation.",
            },
            {
                "question": "Compared to a system that uses a separate high-level planner (LLM) and low-level controller (diffusion policy), what is the key architectural advantage of MoTVLA's unified approach?",
                "choices": [
                    "MoTVLA requires less training data",
                    "MoTVLA avoids the information bottleneck at the planner-controller interface — both pathways share representations and can flexibly exchange information without committing to a fixed communication protocol",
                    "MoTVLA is always faster than modular systems",
                    "MoTVLA doesn't need language conditioning",
                ],
                "correct_index": 1,
                "explanation": "In modular systems, the planner must communicate its intent to the controller through a fixed interface (e.g., subgoal images, waypoints, text). This bottleneck discards information that might be useful. MoTVLA's unified model allows the slow reasoning pathway to influence action prediction through shared hidden states — a much richer communication channel. The fast pathway also benefits from representations learned during slow reasoning training.",
                "why_it_matters": "The monolithic vs. modular debate is central to robotics system design. Modular systems are more interpretable and composable but suffer from information bottlenecks at interfaces. End-to-end systems avoid these bottlenecks but are harder to debug and may be less sample-efficient. MoTVLA represents an interesting middle ground, and understanding its trade-offs helps you make informed architectural decisions.",
            },
            {
                "question": "MoTVLA's dual-process design means most timesteps use the fast pathway. What potential failure mode does this create during long-horizon tasks?",
                "choices": [
                    "The fast pathway will always produce lower-quality actions",
                    "The model may accumulate small errors during extended fast-pathway execution and fail to trigger slow reasoning until the situation has deteriorated beyond recovery",
                    "The fast pathway cannot process language instructions",
                    "Long-horizon tasks always require slow reasoning at every step",
                ],
                "correct_index": 1,
                "explanation": "The fast pathway handles routine execution well, but small errors compound over time. The danger is that each individual error is too subtle to trigger the router's novelty/complexity threshold, but their cumulative effect drifts the robot into an unrecoverable state. By the time the situation is clearly 'novel' enough to trigger slow reasoning, it may be too late. This is analogous to the 'boiling frog' problem in monitoring systems.",
                "why_it_matters": "Error accumulation in autonomous systems is a critical safety concern. This question tests whether you understand that adaptive computation systems can fail not just on individual hard decisions, but through the gradual accumulation of easy-looking decisions. Any system with a 'skip the expensive check' option — whether in robotics, autonomous driving, or financial trading — faces this failure mode.",
            },
        ],
    },
    # ── #23  SpatialVLM ────────────────────────────────────────────────
    {
        "number": 23,
        "summary": (
            "SpatialVLM trains Vision-Language Models to reason about 3D spatial relationships "
            "from 2D images by constructing a large-scale dataset of spatial reasoning QA pairs. "
            "Using metric depth estimation and object detection, the system automatically generates "
            "training data about distances, sizes, and spatial relations between objects. The resulting "
            "model can answer spatial questions and support robotic manipulation through spatial reasoning."
        ),
        "key_learnings": (
            "- SpatialVLM addresses a key limitation of standard VLMs: they lack quantitative 3D spatial understanding despite processing 2D images\n"
            "- Training data is generated automatically by combining monocular depth estimation with object detection to extract 3D spatial relationships\n"
            "- The model learns to reason about metric distances, relative sizes, and spatial relations (above, behind, left-of) from 2D images alone\n"
            "- Spatial reasoning enables downstream robotic tasks like specifying manipulation targets relative to other objects\n"
            "- The data generation pipeline is the key contribution: scaling spatial QA data is what enables the capability, not architectural changes"
        ),
        "reading_guide": (
            "Start with Section 1 for the motivation: VLMs can describe scenes but cannot reason quantitatively about spatial relationships. "
            "Section 3 on the data generation pipeline is the most important — understand how depth estimation and object detection produce spatial QA pairs. "
            "Pay attention to data quality: how noise in depth estimates propagates to training labels. "
            "Section 4 on results should focus on the spatial reasoning benchmarks and the robotic application. "
            "Compare with SpatialBot (#24) to understand different approaches to the same problem."
        ),
        "questions": [
            {
                "question": "SpatialVLM generates spatial reasoning training data using monocular depth estimation. What systematic bias does this introduce?",
                "choices": [
                    "Monocular depth models have no systematic biases",
                    "Depth estimation errors are larger for distant objects and in textureless regions, causing the model to learn biased spatial relationships that are more accurate for nearby, textured objects",
                    "Monocular depth always underestimates distances",
                    "The bias only affects indoor scenes, not outdoor ones",
                ],
                "correct_index": 1,
                "explanation": "Monocular depth estimation has well-known failure modes: it's less accurate for distant objects (where small pixel errors map to large metric errors), textureless surfaces (where there are no visual cues for depth), and reflective/transparent objects. Since SpatialVLM's training data is derived from these depth estimates, the model inherits these biases — it will be more confident and accurate about spatial relationships between nearby, textured objects and less reliable for distant or textureless scenes.",
                "why_it_matters": "Understanding how upstream model biases propagate through data generation pipelines is essential for evaluating any system built on synthetic/derived training data. This is increasingly common in robotics (simulation-to-real, LLM-generated annotations, depth-derived labels). If you don't understand the bias profile of your data source, you can't predict where your downstream model will fail.",
            },
            {
                "question": "SpatialVLM achieves spatial reasoning without architectural changes — only through training data. What does this imply about the role of architecture vs. data in spatial understanding?",
                "choices": [
                    "Architecture never matters for spatial reasoning",
                    "Standard VLM architectures already have sufficient representational capacity for spatial reasoning; the bottleneck was the absence of spatial supervision in pre-training data, not a fundamental architectural limitation",
                    "The data is simply memorized and the model cannot generalize to novel spatial queries",
                    "This approach only works because the model was pre-trained on ImageNet",
                ],
                "correct_index": 1,
                "explanation": "SpatialVLM's key insight is that standard VLM architectures (ViT + LLM) can represent spatial relationships — they just never learned to because their training data (image captions, VQA) rarely contains quantitative spatial information. The bottleneck was a data gap, not an architectural gap. This is a powerful finding: it means spatial reasoning can be added to existing VLMs through fine-tuning rather than requiring new architectures.",
                "why_it_matters": "This speaks to a fundamental question in ML: when a model fails at a task, is it because the architecture can't represent the solution, or because the training data doesn't teach it? SpatialVLM provides evidence that many VLM 'failures' are data failures, not architectural ones. This has practical implications: before redesigning your architecture, check whether better training data could solve the problem.",
            },
            {
                "question": "For robotic manipulation, SpatialVLM enables spatial language commands like 'pick up the object 10cm to the left of the cup.' What is the main limitation of using a VLM for this rather than a traditional perception pipeline with explicit 3D reconstruction?",
                "choices": [
                    "VLMs cannot process images fast enough for real-time control",
                    "VLMs produce probabilistic spatial estimates without calibrated uncertainty, whereas explicit 3D reconstruction provides geometric measurements with known error bounds that can be propagated through the planning pipeline",
                    "VLMs require more expensive hardware than traditional perception",
                    "Traditional perception pipelines are always more accurate",
                ],
                "correct_index": 1,
                "explanation": "A traditional pipeline (stereo cameras → point cloud → segmentation → pose estimation) provides metric measurements with quantifiable uncertainty at each stage. A VLM's spatial estimate is a point prediction without calibrated confidence — you don't know if '10cm' has ±1cm or ±5cm error. For manipulation, error bounds matter: a grasp planner needs to know the uncertainty to plan robust grasps. VLMs lack this uncertainty quantification.",
                "why_it_matters": "The trade-off between learned perception (flexible, handles novel objects) and geometric perception (precise, uncertainty-aware) is central to modern robotics. Understanding when each is appropriate — and what's lost when switching to learned approaches — is essential for building reliable systems. This question tests whether you think critically about what's sacrificed when replacing engineered pipelines with learned models.",
            },
        ],
    },
    # ── #24  SpatialBot ────────────────────────────────────────────────
    {
        "number": 24,
        "summary": (
            "SpatialBot enhances robotic manipulation by incorporating depth estimation and "
            "spatial feature representations into a VLM-based robot controller. By explicitly "
            "processing depth information alongside RGB images, the model achieves better spatial "
            "understanding for manipulation tasks. The system demonstrates improved performance "
            "on tasks requiring precise spatial reasoning about object positions and relationships."
        ),
        "key_learnings": (
            "- SpatialBot explicitly incorporates depth features into the VLM pipeline, rather than relying on implicit depth understanding from RGB alone\n"
            "- Depth estimation provides a direct signal about 3D structure that complements the semantic information from RGB images\n"
            "- The model fuses depth and RGB features at the representation level, allowing joint spatial-semantic reasoning\n"
            "- Compared to SpatialVLM (#23) which uses only data augmentation, SpatialBot modifies the input representation to include depth\n"
            "- The approach shows that explicit depth input improves manipulation accuracy, especially for tasks involving precise placement and stacking"
        ),
        "reading_guide": (
            "Start with the Introduction to understand why RGB-only VLMs struggle with depth-dependent manipulation tasks. "
            "Focus on the architecture section to understand how depth features are extracted and fused with RGB features. "
            "Compare the depth fusion strategy with SpatialVLM's data-only approach — this contrast is instructive. "
            "In experiments, pay attention to which task categories benefit most from depth input (typically precision placement tasks). "
            "The ablation on depth quality (estimated vs. ground-truth) reveals practical deployment considerations."
        ),
        "questions": [
            {
                "question": "SpatialBot uses estimated depth as an additional input modality. In what scenario would this actually hurt performance compared to an RGB-only model?",
                "choices": [
                    "When the robot has a high-resolution camera",
                    "When the depth estimation model produces systematic errors on the target domain (e.g., transparent objects, reflective surfaces) that the RGB-only model would handle correctly through learned implicit depth cues",
                    "When the task requires language understanding",
                    "When the robot operates in a well-lit environment",
                ],
                "correct_index": 1,
                "explanation": "If the depth estimator systematically fails on certain objects (glass, mirrors, shiny metal), the erroneous depth signal actively misleads the model. An RGB-only model might handle these objects correctly using learned monocular cues (texture gradients, occlusion, familiar object sizes). Adding a bad depth input is worse than no depth input because the model learns to trust the depth channel, and when it's wrong, the model's spatial reasoning is corrupted.",
                "why_it_matters": "This illustrates a general principle in multi-modal learning: adding a modality only helps if that modality's signal-to-noise ratio is positive for the target domain. In real-world deployment, depth sensors and estimators have specific failure modes (transparent objects, bright sunlight, specific materials). Understanding when additional modalities help versus hurt is critical for robust system design.",
            },
            {
                "question": "SpatialBot fuses depth features with RGB features at the representation level. What is the key advantage of this over late fusion (processing RGB and depth separately then combining their action predictions)?",
                "choices": [
                    "Early fusion uses less memory",
                    "Early/mid fusion allows the model to learn cross-modal correlations — for example, using RGB texture to disambiguate noisy depth estimates, or using depth boundaries to sharpen object segmentation in RGB",
                    "Early fusion is always faster at inference time",
                    "Late fusion cannot handle language conditioning",
                ],
                "correct_index": 1,
                "explanation": "Representation-level fusion enables cross-modal interaction: depth can help resolve RGB ambiguities (is that a real object or a photo?) and RGB can help resolve depth ambiguities (noisy depth near edges can be cleaned up using RGB edge information). Late fusion treats modalities as independent, missing these synergistic interactions. For manipulation, where spatial-semantic interactions matter (grasp the blue object at this depth), early fusion is particularly important.",
                "why_it_matters": "Fusion strategy is a fundamental design choice in multi-modal architectures. It applies not just to RGB-D fusion but to any multi-modal system (vision-language, vision-tactile, multi-sensor). Understanding the trade-offs between early, mid, and late fusion — and when cross-modal interactions matter — is essential knowledge for designing any multi-modal learning system.",
            },
            {
                "question": "Comparing SpatialBot's approach (explicit depth input) with SpatialVLM's approach (spatial QA training data), which strategy scales better to new robotic domains?",
                "choices": [
                    "SpatialBot scales better because depth sensors are universally available",
                    "SpatialVLM scales better because depth estimation is unreliable",
                    "SpatialVLM's data-centric approach scales better because it doesn't require architectural changes or depth-specific input processing, making it easier to apply to any VLM backbone or new domain",
                    "Both scale equally well",
                ],
                "correct_index": 2,
                "explanation": "SpatialVLM's data-only approach is more portable: you can apply it to any VLM by fine-tuning on spatial QA data without changing the architecture. SpatialBot requires modifying the input pipeline to accept depth, retraining the fusion layers, and ensuring depth estimation quality in the new domain. When moving to new cameras, new environments, or new VLM backbones, the data-centric approach requires less re-engineering.",
                "why_it_matters": "Scalability and portability are often more important than raw performance in practice. This question tests whether you can evaluate approaches not just on accuracy but on practical deployment considerations. The trend toward data-centric AI (improving data rather than architectures) is partly motivated by this scalability argument, and understanding the trade-offs helps you choose the right approach for your specific constraints.",
            },
        ],
    },
    # ── #25  3D-VLA ────────────────────────────────────────────────────
    {
        "number": 25,
        "summary": (
            "3D-VLA integrates 3D visual representations — point clouds and voxels — into the "
            "Vision-Language-Action framework, replacing or augmenting 2D image inputs. By operating "
            "on explicit 3D geometry, the model achieves better spatial understanding for manipulation "
            "tasks. The approach generates 3D-grounded action predictions and demonstrates improved "
            "performance on tasks requiring precise 3D spatial reasoning."
        ),
        "key_learnings": (
            "- 3D-VLA replaces 2D image inputs with 3D representations (point clouds or voxels), providing explicit geometric structure to the VLA\n"
            "- 3D representations naturally encode depth, occlusion, and spatial relationships that 2D images only capture implicitly\n"
            "- The model uses 3D-aware tokenization to convert point clouds into tokens compatible with the language model backbone\n"
            "- 3D inputs improve robustness to camera viewpoint changes because the 3D structure is view-invariant\n"
            "- The trade-off is that 3D representations require depth sensors or multi-view reconstruction, adding hardware or computational requirements"
        ),
        "reading_guide": (
            "Begin with the Introduction to understand why 2D-to-3D lifting is lossy and how explicit 3D inputs help. "
            "Focus on the 3D tokenization approach in the Method section — how point clouds are converted to tokens is the key technical contribution. "
            "Compare with standard VLAs that process 2D images to understand what information is gained and lost. "
            "In experiments, focus on viewpoint robustness results and tasks requiring 3D reasoning. "
            "The comparison with 2D-VLA baselines on the same tasks quantifies the value of 3D input."
        ),
        "questions": [
            {
                "question": "3D-VLA tokenizes point clouds for input to a language model backbone. What is the fundamental challenge this tokenization must solve that 2D image tokenization (patch embedding) does not face?",
                "choices": [
                    "Point clouds are always larger than images",
                    "Point clouds are unordered, irregularly sampled, and have variable density — unlike images which have a fixed regular grid structure that convolutions and patch embeddings naturally exploit",
                    "Point clouds cannot represent color information",
                    "Language models cannot process non-text tokens",
                ],
                "correct_index": 1,
                "explanation": "Images have a fixed grid structure: every image is H×W pixels with regular spacing. Patch embedding simply divides this grid into non-overlapping patches. Point clouds have none of this structure: points are unordered (permutation invariant), irregularly spaced (denser near surfaces, sparser elsewhere), and variable in count. Tokenization must handle all of this while producing a fixed-size set of tokens that captures the 3D geometry.",
                "why_it_matters": "This connects to a fundamental question in geometric deep learning: how to process unstructured data. The same challenge appears in processing molecules, social networks, and meshes. Understanding why point clouds are harder to tokenize than images helps you evaluate 3D architectures and appreciate why PointNet, point cloud Transformers, and voxelization each make different trade-offs.",
            },
            {
                "question": "3D-VLA loses access to the rich visual pre-training that 2D VLMs benefit from (e.g., CLIP, DINOv2 trained on billions of images). How does this affect the model's capabilities?",
                "choices": [
                    "It doesn't matter because 3D understanding is all that's needed for manipulation",
                    "The model gains spatial precision but loses semantic richness — it may understand object geometry well but struggle with recognizing object categories, attributes, or following complex language instructions that benefit from web-scale visual-semantic pre-training",
                    "3D pre-training is already as mature as 2D pre-training",
                    "The language model backbone compensates entirely for the lack of visual pre-training",
                ],
                "correct_index": 1,
                "explanation": "2D visual encoders like CLIP have been trained on billions of image-text pairs, learning rich semantic features that enable zero-shot recognition, attribute understanding, and language grounding. 3D encoders lack this scale of pre-training — there are orders of magnitude fewer 3D datasets. So 3D-VLA trades semantic richness for geometric precision. Tasks like 'pick up the ripe banana' (requiring semantic understanding) may suffer, while 'insert the peg into the hole' (requiring geometric precision) benefits.",
                "why_it_matters": "Pre-training data availability is one of the most important practical constraints in ML. The 2D vision ecosystem has an enormous advantage in pre-training scale, which is why many approaches prefer to lift 2D features to 3D rather than process 3D directly. Understanding this ecosystem asymmetry helps you make informed decisions about when the geometric benefits of 3D outweigh the semantic benefits of 2D pre-training.",
            },
            {
                "question": "When would you choose 3D-VLA over a 2D VLA with estimated depth (like SpatialBot #24)?",
                "choices": [
                    "Always, because 3D is strictly better than 2D",
                    "When the task involves heavy occlusion, multi-object reasoning in clutter, or precise 3D alignment where estimated depth from a single view is insufficient and multi-view 3D reconstruction provides critical geometric information",
                    "When you have a limited compute budget",
                    "When the task requires understanding object textures and materials",
                ],
                "correct_index": 1,
                "explanation": "Single-view depth estimation struggles with occlusions (can't see behind objects), dense clutter (objects blocking each other), and precise alignment tasks (where sub-centimeter 3D accuracy matters). Full 3D reconstruction from multiple views or depth sensors resolves these issues. The overhead of 3D processing is justified when the task requires geometric information that single-view depth cannot provide.",
                "why_it_matters": "Choosing the right representation for the task is a critical engineering decision. 3D processing adds hardware cost, computational overhead, and engineering complexity. Understanding exactly when these costs are justified — and when cheaper 2D alternatives suffice — is essential for building practical robotic systems that balance performance with resource constraints.",
            },
        ],
    },
    # ── #26  DP3 (3D Diffusion Policy) ─────────────────────────────────
    {
        "number": 26,
        "summary": (
            "3D Diffusion Policy (DP3) extends diffusion-based action generation to 3D point "
            "cloud observations, combining the multimodal action distribution modeling of diffusion "
            "policies with the viewpoint-invariant spatial understanding of 3D representations. "
            "By conditioning diffusion on 3D point clouds rather than 2D images, DP3 achieves "
            "significantly improved robustness to camera viewpoint changes and better spatial "
            "precision for manipulation tasks."
        ),
        "key_learnings": (
            "- DP3 combines two powerful ideas: diffusion policy (for multimodal action distributions) and 3D point cloud inputs (for viewpoint-invariant spatial understanding)\n"
            "- The policy is conditioned on point cloud observations processed by a 3D encoder, replacing the 2D image encoder in standard diffusion policies\n"
            "- 3D inputs provide natural robustness to camera viewpoint changes — a major failure mode of 2D diffusion policies\n"
            "- DP3 maintains the benefits of diffusion policies: handling multimodal action distributions and generating smooth action trajectories\n"
            "- The approach is significantly more sample-efficient than 2D alternatives on tasks with viewpoint variation in training data"
        ),
        "reading_guide": (
            "Start with the Introduction to understand the two failure modes being addressed: unimodal action prediction (solved by diffusion) and viewpoint sensitivity (solved by 3D). "
            "In the Method section, focus on how point cloud conditioning integrates with the diffusion denoising process. "
            "Pay special attention to the 3D encoder architecture and how it produces conditioning vectors for the diffusion model. "
            "In experiments, the viewpoint robustness results are the most important — compare with 2D baselines at novel camera angles. "
            "The sample efficiency analysis reveals when 3D inputs provide the largest advantage."
        ),
        "questions": [
            {
                "question": "DP3 conditions a diffusion policy on 3D point clouds. Why does this combination specifically address a limitation that neither diffusion policies alone nor 3D representations alone would solve?",
                "choices": [
                    "Diffusion adds speed and 3D adds accuracy — they are independent improvements",
                    "Diffusion policies model multimodal action distributions (multiple valid ways to grasp), while 3D inputs provide the viewpoint-invariant spatial precision needed to execute each mode correctly — without 3D, the diffusion model's action modes may be spatially imprecise, and without diffusion, the 3D model can't represent alternative strategies",
                    "3D point clouds require diffusion to process efficiently",
                    "Diffusion policies cannot work with 2D images",
                ],
                "correct_index": 1,
                "explanation": "The two components solve complementary problems. A 3D policy without diffusion might precisely localize a grasp point but can only predict one approach direction (unimodal). A diffusion policy without 3D can model multiple approach strategies but may fail when the camera moves because its spatial predictions are view-dependent. Together, the model can represent multiple spatially-precise manipulation strategies that are consistent across viewpoints.",
                "why_it_matters": "Understanding why components are complementary — rather than independently beneficial — is key to evaluating combined approaches. In ML, many papers combine techniques (Transformer + diffusion, 3D + language, etc.), and the critical question is whether the combination provides synergy or just marginal additive improvement. DP3 is a good example of genuine synergy.",
            },
            {
                "question": "DP3 processes point clouds from depth cameras. In practice, what is the most significant deployment challenge compared to a 2D diffusion policy?",
                "choices": [
                    "Diffusion denoising is slower with 3D conditioning",
                    "Point cloud acquisition and preprocessing (noise filtering, downsampling, registration across multiple views) introduces latency and failure modes that don't exist with RGB cameras, and depth sensor quality degrades with certain materials and lighting conditions",
                    "3D diffusion policies require more training data",
                    "Point clouds cannot represent object color or texture",
                ],
                "correct_index": 1,
                "explanation": "RGB cameras are cheap, reliable, and produce clean images in most conditions. Depth sensors (structured light, ToF, stereo) are more expensive, produce noisy data requiring preprocessing, fail on specific materials (transparent, reflective, very dark), and may need calibration. Multi-view 3D reconstruction adds multi-camera synchronization and registration challenges. These practical engineering issues often dominate theoretical performance gains.",
                "why_it_matters": "The gap between research benchmarks and real-world deployment is often dominated by practical engineering challenges rather than algorithmic limitations. Understanding the full deployment pipeline — including sensor requirements, preprocessing, calibration, and failure modes — is essential for translating research into working systems. This question tests whether you think beyond algorithmic performance.",
            },
            {
                "question": "DP3 is more sample-efficient than 2D diffusion policies when training data includes viewpoint variation. Why does 3D representation provide this efficiency gain?",
                "choices": [
                    "3D encoders have fewer parameters than 2D encoders",
                    "3D representations factor out viewpoint as a nuisance variable — the model doesn't need to spend capacity learning that different 2D views correspond to the same 3D scene, which a 2D model must learn from data",
                    "Point clouds contain more information per sample than images",
                    "3D diffusion uses fewer denoising steps",
                ],
                "correct_index": 1,
                "explanation": "When using 2D inputs, the model must learn viewpoint invariance from data: it needs many examples from different angles to understand that the same 3D arrangement produces different 2D images. 3D representations encode this invariance architecturally — the same scene always produces the same point cloud regardless of viewpoint. This frees up model capacity and reduces the data needed to learn the underlying manipulation skill.",
                "why_it_matters": "Inductive bias and sample efficiency are deeply connected. When you encode task-relevant invariances into the representation (viewpoint invariance via 3D, translation invariance via convolutions), you reduce the data needed to learn the task. Understanding which invariances matter for which tasks — and how to encode them — is one of the most important skills in applied ML.",
            },
        ],
    },
    # ── #27  Act3D ─────────────────────────────────────────────────────
    {
        "number": 27,
        "summary": (
            "Act3D uses 3D Feature Field Transformers for multi-task robotic manipulation, "
            "constructing continuous 3D feature fields from multi-view images and using Transformer "
            "attention to predict precise 3D action locations. Rather than discretizing the workspace "
            "into voxels, Act3D queries arbitrary 3D points, enabling higher spatial precision "
            "with lower memory cost than voxel-based approaches like PerAct."
        ),
        "key_learnings": (
            "- Act3D constructs continuous 3D feature fields rather than discrete voxel grids, enabling arbitrary-resolution spatial queries\n"
            "- The model uses a coarse-to-fine strategy: first identifying a rough region of interest, then refining with higher-resolution queries\n"
            "- Transformer cross-attention between 3D query points and multi-view image features produces the 3D feature field\n"
            "- This approach avoids the cubic memory scaling of voxel representations while achieving higher spatial precision\n"
            "- Act3D demonstrates strong multi-task performance, handling diverse manipulation tasks with a single architecture"
        ),
        "reading_guide": (
            "Start with Section 1 to understand the limitations of voxel-based approaches (PerAct) that motivated Act3D. "
            "The Method section is technical but essential — focus on how 3D feature fields are constructed via cross-attention from multi-view images. "
            "Understand the coarse-to-fine querying strategy, which is the key to achieving high precision without high memory cost. "
            "Compare directly with PerAct (#29) in the experiments to understand the precision vs. efficiency trade-off. "
            "The multi-task results show where continuous representations outperform discrete ones."
        ),
        "questions": [
            {
                "question": "Act3D uses continuous 3D feature fields while PerAct uses discrete voxel grids. What is the fundamental computational advantage of Act3D's approach?",
                "choices": [
                    "Continuous fields use fewer parameters",
                    "Continuous fields can be queried at arbitrary resolution in regions of interest without allocating memory for the entire workspace — voxels must uniformly discretize the whole space, trading resolution for memory",
                    "Continuous fields are faster to construct than voxel grids",
                    "Continuous fields naturally handle dynamic environments",
                ],
                "correct_index": 1,
                "explanation": "Voxel grids uniformly allocate memory across the entire workspace: doubling resolution in each dimension increases memory by 8x (cubic scaling). Act3D's continuous field can be queried densely only where needed (near the manipulation target) and sparsely elsewhere. The coarse-to-fine strategy makes this practical: first query coarsely to find the region of interest, then query finely within that region. Total memory is proportional to the number of queries, not the workspace volume.",
                "why_it_matters": "The continuous vs. discrete representation trade-off appears throughout 3D computer vision and robotics (NeRFs vs. voxels, implicit surfaces vs. meshes). Understanding the computational implications — especially memory scaling — helps you choose the right representation for your task and workspace size. For large workspaces requiring fine precision, discrete approaches become impractical.",
            },
            {
                "question": "Act3D's coarse-to-fine querying strategy introduces a potential failure mode. What is it?",
                "choices": [
                    "Fine queries are too slow for real-time control",
                    "If the coarse stage misidentifies the region of interest, the fine stage refines within the wrong region and produces a precise but incorrect action — errors in coarse localization cannot be recovered by fine refinement",
                    "The coarse-to-fine strategy cannot handle multiple objects",
                    "Fine-grained queries produce noisy features",
                ],
                "correct_index": 1,
                "explanation": "The coarse-to-fine approach is a form of cascaded decision-making: the fine stage's search space is constrained by the coarse stage's output. If the coarse stage makes an error — perhaps attending to a distractor object instead of the target — the fine stage will precisely localize a point on the wrong object. This is a well-known problem in hierarchical search: early errors propagate irreversibly.",
                "why_it_matters": "Cascaded/hierarchical architectures are common in ML (two-stage object detectors, coarse-to-fine generation, hierarchical planning). They all share this failure mode: early-stage errors propagate and compound. Understanding this helps you design appropriate error recovery mechanisms and evaluate whether the efficiency gains of hierarchical processing justify the added fragility.",
            },
            {
                "question": "Act3D constructs 3D features from multi-view 2D images via cross-attention. Compared to 3D-VLA (#25) which processes point clouds directly, what does Act3D gain and lose?",
                "choices": [
                    "Act3D gains pre-trained 2D visual features (from CLIP/ImageNet) and semantic richness, but loses the explicit geometric structure that point clouds provide and must reconstruct 3D from 2D via learned attention",
                    "Act3D is strictly better because it avoids noisy depth sensors",
                    "Act3D loses the ability to reason about 3D structure",
                    "Act3D gains speed but loses accuracy",
                ],
                "correct_index": 0,
                "explanation": "Act3D processes 2D images with pre-trained visual encoders, inheriting rich semantic features from web-scale pre-training. It then reconstructs 3D through cross-attention — a learned, implicit process. 3D-VLA starts with explicit 3D geometry but lacks comparable pre-trained features. Act3D's 3D reconstruction is approximate and may fail in challenging geometric situations, while 3D-VLA's explicit point clouds provide ground-truth geometry but weaker semantics.",
                "why_it_matters": "This 2D-to-3D lifting vs. direct 3D processing trade-off is fundamental in robotics vision. The 2D path leverages the massive pre-training ecosystem but must reconstruct 3D. The 3D path starts with geometry but lacks pre-trained semantics. Understanding this trade-off helps you choose the right approach based on your task's requirements (semantic understanding vs. geometric precision) and available resources (pre-trained models vs. 3D sensors).",
            },
        ],
    },
    # ── #28  RVT-2 ─────────────────────────────────────────────────────
    {
        "number": 28,
        "summary": (
            "RVT-2 learns precise multi-task manipulation from very few demonstrations using "
            "multi-view rendering and virtual keypoint detection. The model renders the scene from "
            "multiple virtual viewpoints, detects keypoints (action targets) in each view, and "
            "triangulates them to obtain precise 3D action locations. RVT-2 improves upon RVT with "
            "better efficiency and accuracy through architectural refinements."
        ),
        "key_learnings": (
            "- RVT-2 re-renders the scene from multiple canonical virtual viewpoints, decoupling the action prediction from the physical camera placement\n"
            "- Keypoint detection in re-rendered views is more tractable than direct 3D action prediction because it leverages mature 2D detection methods\n"
            "- Multi-view keypoint triangulation recovers precise 3D action locations without explicit 3D representation (no voxels or point clouds)\n"
            "- The approach is highly sample-efficient, learning multi-task manipulation from as few as ~10 demonstrations per task\n"
            "- RVT-2 achieves competitive or superior performance to voxel-based methods (PerAct) with significantly less computation"
        ),
        "reading_guide": (
            "Start with the Introduction to understand the re-rendering insight: why virtual viewpoints help. "
            "In the Method section, focus on the multi-view rendering pipeline and how keypoints are detected and triangulated. "
            "Compare with RVT (the predecessor) to understand what architectural improvements RVT-2 makes. "
            "The few-shot learning results are particularly important — understand how sample efficiency relates to the representation choice. "
            "Compare with PerAct (#29) and Act3D (#27) on the RLBench benchmark to understand relative strengths."
        ),
        "questions": [
            {
                "question": "RVT-2 re-renders scenes from virtual canonical viewpoints before detecting keypoints. Why does this re-rendering step provide such a large benefit for few-shot learning?",
                "choices": [
                    "Re-rendering produces higher-resolution images",
                    "Canonical viewpoints eliminate viewpoint variation from the learning problem — the model always sees the scene from the same angles, so it doesn't need to learn viewpoint invariance from limited data",
                    "Re-rendering removes background clutter",
                    "Virtual viewpoints provide depth information that real cameras lack",
                ],
                "correct_index": 1,
                "explanation": "With only ~10 demonstrations, the model cannot afford to spend capacity learning that different viewpoints show the same scene. By re-rendering to fixed canonical viewpoints, viewpoint variation is eliminated architecturally. The model can focus entirely on learning the task-relevant spatial patterns. This is particularly powerful for few-shot settings where every bit of inductive bias matters.",
                "why_it_matters": "This illustrates a powerful principle: when data is scarce, encode as many invariances as possible into the architecture or preprocessing. The choice to re-render from canonical views is equivalent to building viewpoint invariance into the data pipeline. Understanding this principle helps you design sample-efficient learning systems by identifying which variations are task-irrelevant and can be factored out.",
            },
            {
                "question": "RVT-2 decomposes 3D action prediction into 2D keypoint detection + triangulation. What precision limitation does this decomposition introduce?",
                "choices": [
                    "2D keypoint detection always produces integer pixel coordinates",
                    "Triangulation accuracy depends on the geometric configuration of virtual viewpoints — actions along directions where views are nearly parallel (small baseline) will have poor depth precision, and keypoint detection errors are amplified by the triangulation geometry",
                    "Keypoints cannot represent rotational actions",
                    "Triangulation requires exact camera calibration which is impossible in simulation",
                ],
                "correct_index": 1,
                "explanation": "Triangulation precision is governed by the geometry of the multi-view setup: the depth resolution is inversely proportional to the baseline (distance between viewpoints) and proportional to the pixel error. If two views are nearly parallel, a 1-pixel error in each view translates to a large 3D error along the depth direction. The choice of virtual viewpoints thus directly determines where in the workspace precision is highest and lowest.",
                "why_it_matters": "Multi-view triangulation is a fundamental geometric concept that appears throughout 3D vision and robotics. Understanding its precision characteristics — and how they depend on camera geometry — is essential for any system that recovers 3D from multiple 2D observations. This knowledge applies to stereo vision, structure-from-motion, and any multi-view learning system.",
            },
            {
                "question": "RVT-2 avoids explicit 3D representations (voxels, point clouds) entirely, instead using multi-view 2D processing. When would this be clearly inferior to a voxel-based method like PerAct?",
                "choices": [
                    "When the task requires color perception",
                    "When the workspace contains severe occlusions where no single 2D viewpoint can see the action target — voxel representations can aggregate partial observations across views into a complete 3D understanding that 2D keypoint detection in individual views cannot achieve",
                    "When the robot has a single camera",
                    "When the task involves deformable objects",
                ],
                "correct_index": 1,
                "explanation": "RVT-2 detects keypoints independently in each rendered view. If the action target is occluded in most views (e.g., reaching into a cluttered shelf), no single view provides a clean detection target. Voxel-based methods like PerAct aggregate information across all views into a unified 3D representation, which can reason about the occluded target using partial observations from multiple angles combined in 3D space.",
                "why_it_matters": "Understanding the occlusion handling capabilities of different 3D representations is critical for real-world deployment. Tabletop manipulation has relatively few occlusions, but real-world environments (shelves, drawers, cluttered bins) have severe occlusions. The representation you choose determines how well your system handles these challenging but common scenarios.",
            },
        ],
    },
    # ── #29  PerAct ────────────────────────────────────────────────────
    {
        "number": 29,
        "summary": (
            "PerAct (Perceiver-Actor) addresses multi-task robotic manipulation by voxelizing the "
            "workspace into a 3D grid and using a Perceiver Transformer to predict action keypoints "
            "directly in 3D voxel space. Language instructions are encoded and cross-attended with "
            "voxel features, enabling the model to perform diverse manipulation tasks specified by "
            "natural language. PerAct demonstrated strong multi-task performance on the RLBench benchmark."
        ),
        "key_learnings": (
            "- PerAct voxelizes the workspace into a 3D grid and processes it with a Perceiver Transformer, unifying perception and action in 3D\n"
            "- Actions are predicted as 3D keypoints (voxel coordinates) plus discrete rotation and gripper state, avoiding continuous regression\n"
            "- The Perceiver architecture handles the large number of voxel tokens through cross-attention with a fixed set of latent tokens\n"
            "- Language conditioning via cross-attention enables multi-task learning from language instructions\n"
            "- PerAct set the initial benchmark for multi-task manipulation on RLBench, motivating subsequent methods like Act3D and RVT-2"
        ),
        "reading_guide": (
            "Section 1 provides essential context for the multi-task manipulation problem and why 3D representations help. "
            "Focus on Section 3 (Method): understand the voxelization process, how the Perceiver processes voxels, and how actions are decoded. "
            "The action representation (keypoints + discretized rotation) is a key design choice — understand why continuous regression was avoided. "
            "In experiments, the RLBench multi-task results are the primary contribution. "
            "Compare with Act3D (#27) and RVT-2 (#28) to understand where voxelization is a bottleneck."
        ),
        "questions": [
            {
                "question": "PerAct uses a Perceiver Transformer to process voxel representations. Why is the Perceiver architecture specifically well-suited for this task compared to a standard Transformer?",
                "choices": [
                    "Perceivers are faster than standard Transformers at all input sizes",
                    "The Perceiver's cross-attention from inputs to a fixed set of latent tokens provides linear scaling with voxel count, whereas standard self-attention over all voxels would scale quadratically and be infeasible for even moderate 3D resolutions",
                    "Perceivers can process multi-modal inputs while standard Transformers cannot",
                    "Perceivers have built-in 3D positional encodings",
                ],
                "correct_index": 1,
                "explanation": "A 100³ voxel grid has 1 million tokens. Self-attention over 1M tokens is computationally infeasible (O(n²) = 10¹² operations). The Perceiver uses cross-attention from the 1M input tokens to a much smaller set of latent tokens (e.g., 512), reducing the cost to O(n·m) where m << n. This makes processing high-resolution voxel grids tractable while still allowing the model to attend to any spatial location.",
                "why_it_matters": "The Perceiver architecture is a general solution to the 'too many tokens' problem that appears throughout multi-modal and 3D ML. Understanding why it's needed — and what it sacrifices (direct token-to-token attention) — helps you evaluate when to use Perceivers vs. standard Transformers vs. other efficient attention mechanisms. This computational constraint shapes many architectural decisions in 3D robotics.",
            },
            {
                "question": "PerAct discretizes both the workspace (voxels) and rotations (discrete bins). What is the key consequence of this double discretization for manipulation precision?",
                "choices": [
                    "It makes training faster but doesn't affect precision",
                    "The achievable precision is bounded by the discretization resolution — at 100³ voxels over a 1m³ workspace, each voxel is 1cm³, setting a hard ceiling on positional accuracy that compounds with discrete rotation error for tasks like peg insertion or connector mating",
                    "Discretization only affects computational cost, not task performance",
                    "The precision loss is negligible for all practical manipulation tasks",
                ],
                "correct_index": 1,
                "explanation": "Voxel size sets a hard precision limit: 100³ voxels over a 1m workspace gives 1cm resolution. Many manipulation tasks (USB insertion, key turning, precision assembly) require sub-millimeter accuracy. Similarly, discretizing rotations into bins (e.g., 72 bins = 5° resolution) limits orientation accuracy. These errors compound: being 1cm off in position AND 5° off in rotation can cause a grasp to fail entirely. This motivated Act3D's continuous approach.",
                "why_it_matters": "Discretization resolution is a fundamental design parameter that creates a precision-memory trade-off. Finer voxels give better precision but cubic memory growth. This trade-off recurs in any discrete representation (image resolution, action tokens, vocabulary size). Understanding it helps you diagnose why methods fail on precision tasks and whether the solution is better resolution, continuous representations, or coarse-to-fine strategies.",
            },
            {
                "question": "PerAct uses language cross-attention to condition voxel features on task instructions. What is a limitation of this approach compared to methods that use language to condition the action head directly?",
                "choices": [
                    "Cross-attention is slower than direct conditioning",
                    "Language cross-attention in the voxel encoder means the 3D representation is task-specific — the same scene produces different voxel features for different instructions, preventing reuse of perception across tasks and making it harder to share sub-task representations",
                    "Language cross-attention cannot handle complex instructions",
                    "Direct conditioning provides better gradient flow",
                ],
                "correct_index": 1,
                "explanation": "When language modulates the voxel representation early, the 3D features become task-entangled: the model produces different spatial representations for 'pick up the cup' vs. 'push the cup.' This prevents reusing a shared scene representation across tasks. An alternative design — task-agnostic perception followed by language-conditioned action selection — would allow the perception module to be shared and potentially more robust, but might lose the benefit of task-focused attention.",
                "why_it_matters": "Where to inject task conditioning in a multi-task architecture is a recurring design decision. Early injection (in perception) allows task-focused feature extraction but prevents representation sharing. Late injection (in action selection) enables shared perception but may miss task-relevant details. Understanding this trade-off helps you design multi-task systems with the right balance of specialization and sharing.",
            },
        ],
    },
    # ── #30  GNFactor ──────────────────────────────────────────────────
    {
        "number": 30,
        "summary": (
            "GNFactor combines Generalizable Neural Feature Fields with robot learning for "
            "multi-task manipulation. It constructs a neural radiance field (NeRF) from multi-view "
            "images, but instead of rendering RGB colors, it renders semantic feature fields that "
            "capture both appearance and language-grounded semantics. These feature fields serve as "
            "rich 3D representations for policy learning, enabling better generalization across "
            "visual variations."
        ),
        "key_learnings": (
            "- GNFactor uses generalizable NeRFs to construct 3D feature fields rather than standard voxel or point cloud representations\n"
            "- The neural feature field renders semantic features (from CLIP or similar models) at each 3D point, combining geometry with semantics\n"
            "- Generalizable NeRFs can synthesize representations for new scenes without per-scene optimization, unlike original NeRFs\n"
            "- The feature fields provide a view-consistent 3D representation that is robust to visual variations (lighting, texture, background changes)\n"
            "- GNFactor demonstrates that NeRF-derived representations can outperform raw voxel or point cloud inputs for policy learning"
        ),
        "reading_guide": (
            "Start with the Introduction to understand why NeRF-based representations are appealing for robot learning. "
            "In the Method section, focus on two things: (1) how generalizable NeRFs differ from vanilla NeRFs and why this matters for robotics, "
            "and (2) how semantic features are rendered alongside geometry. "
            "The connection between rendered feature volumes and the policy network is the key architectural contribution. "
            "In experiments, focus on generalization results — particularly how GNFactor handles visual domain changes. "
            "Compare with PerAct (#29) to understand the relative value of NeRF-based vs. voxel-based 3D representations."
        ),
        "questions": [
            {
                "question": "GNFactor renders semantic feature fields from CLIP features rather than RGB values. Why is this a significant design choice compared to rendering RGB and then extracting features?",
                "choices": [
                    "CLIP features are smaller than RGB images, saving memory",
                    "Rendering in semantic feature space produces view-consistent semantic representations directly — extracting CLIP features from rendered RGB images would introduce inconsistencies because 2D feature extraction is not guaranteed to be multi-view consistent, leading to flickering or misaligned features across viewpoints",
                    "CLIP features capture depth information that RGB does not",
                    "Rendering CLIP features is computationally faster than rendering RGB",
                ],
                "correct_index": 1,
                "explanation": "If you render RGB from the NeRF and then extract CLIP features from each rendered view, the features may be inconsistent across views — the same 3D point could get different semantic features when seen from different angles because CLIP was trained on single images, not multi-view consistency. By rendering directly in feature space, the NeRF's 3D consistency constraint ensures that semantic features are coherent across all viewpoints, providing a much more stable 3D semantic representation.",
                "why_it_matters": "Multi-view consistency is a subtle but critical property for 3D robot learning. If the same object gets different feature descriptions from different viewpoints, the policy receives inconsistent inputs depending on camera position. Understanding how to ensure consistency in multi-view learned representations is important for any 3D learning system, and GNFactor's approach of rendering in feature space rather than RGB space is an elegant solution.",
            },
            {
                "question": "GNFactor uses 'generalizable' NeRFs that work on new scenes without per-scene optimization. What trade-off does this generalizability introduce compared to per-scene optimized NeRFs?",
                "choices": [
                    "Generalizable NeRFs produce blurrier renderings and less precise geometry because they must amortize across scenes rather than perfectly fitting each scene — they trade reconstruction quality for instant inference on novel scenes",
                    "Generalizable NeRFs are always worse than per-scene NeRFs in every metric",
                    "Generalizable NeRFs require more training data per scene",
                    "Generalizable NeRFs cannot handle dynamic scenes",
                ],
                "correct_index": 0,
                "explanation": "Per-scene NeRFs optimize hundreds of thousands of parameters to perfectly fit one specific scene, achieving very high reconstruction quality. Generalizable NeRFs use a feed-forward network to predict the radiance field from input views — this amortization across scenes sacrifices per-scene fidelity for the ability to handle new scenes instantly. For robotics, this trade-off is usually worthwhile: you need instant inference, and moderate geometric accuracy suffices.",
                "why_it_matters": "The per-scene optimization vs. amortized inference trade-off appears throughout 3D vision (NeRFs, SDFs, shape reconstruction). In robotics, per-scene optimization is usually impractical because scenes change constantly. Understanding this trade-off helps you evaluate when NeRF-based methods are practical for robotics (generalizable variants) vs. when they're research demonstrations (per-scene variants).",
            },
            {
                "question": "GNFactor combines NeRF-derived 3D features with policy learning. Compared to PerAct (#29) which uses raw voxel features, what specific advantage do NeRF-derived features provide for generalization?",
                "choices": [
                    "NeRF features are faster to compute than voxel features",
                    "NeRF-derived features are inherently view-synthesis aware, encoding how appearance changes with viewpoint and lighting — this gives the policy representations that factor out visual nuisance variables that would confuse a policy trained on raw voxel colors",
                    "NeRF features use less memory than voxel grids",
                    "NeRF features can represent arbitrary workspace sizes while voxels cannot",
                ],
                "correct_index": 1,
                "explanation": "NeRFs are trained to explain how a scene looks from all viewpoints and lighting conditions — this forces them to learn representations that separate intrinsic scene properties (shape, material) from extrinsic variables (viewpoint, lighting). When these representations are used for policy learning, the policy inherits this factored structure, making it naturally more robust to visual domain changes than a policy trained on raw voxel colors, which entangle all these factors.",
                "why_it_matters": "Representation disentanglement — separating task-relevant from task-irrelevant factors — is one of the most powerful ideas in representation learning. GNFactor achieves this implicitly through the NeRF training objective. Understanding how different pre-training objectives induce different types of disentanglement helps you choose the right representation learning approach for downstream tasks.",
            },
        ],
    },
]


def main():
    client = get_admin_client()
    for paper in PAPERS:
        num = paper["number"]
        client.table("papers").update({
            "summary": paper["summary"],
            "key_learnings": paper["key_learnings"],
            "reading_guide": paper["reading_guide"],
        }).eq("number", num).execute()

        for q in paper["questions"]:
            client.table("questions").insert({
                "paper_number": num,
                "question": q["question"],
                "choices": json.dumps(q["choices"]),
                "correct_index": q["correct_index"],
                "explanation": q["explanation"],
                "why_it_matters": q["why_it_matters"],
            }).execute()

        print(f"  #{num}: seeded")

    print(f"Done: {len(PAPERS)} papers")


if __name__ == "__main__":
    main()
