#!/usr/bin/env python3
"""Seed peer reviews for papers 1-28."""
from __future__ import annotations

import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).resolve().parent.parent))
from app.db import get_admin_client

REVIEWS = {
    1: """STRENGTHS:
RT-1 was among the first to convincingly demonstrate that a single Transformer-based policy can absorb 130k real-world demonstrations across 700+ tasks and generalize to new instructions, objects, and backgrounds. The use of TokenLearner for efficient visual token compression was a practical and well-motivated design choice that enabled 3Hz real-time control. The paper's emphasis on data diversity over model scale was a genuinely important insight that influenced the entire field. The ablation studies are thorough, clearly showing how performance scales with data variety rather than model size alone. The evaluation on real robots rather than just simulation was rigorous and set a high bar for subsequent work.

LIMITATIONS:
RT-1 is fundamentally limited to single-arm tabletop manipulation with an Everyday Robots mobile manipulator. The 130k demonstrations required for training represent an enormous data collection effort that most labs cannot replicate. The model does not generalize across embodiments at all -- it is tightly coupled to the specific robot hardware it was trained on. The 35M parameter architecture is deliberately small, which caps the model's ability to reason about complex semantics or long-horizon tasks. The action space is discretized into 256 bins per dimension, which limits precision for fine-grained manipulation. Inference at 3Hz is adequate for pick-and-place but too slow for dynamic tasks. The FiLM conditioning approach for language grounding is relatively shallow compared to the deep cross-attention used in later VLAs. There is no mechanism for incorporating feedback or correcting errors mid-execution.

COMMUNITY RECEPTION:
RT-1 was widely regarded as a landmark paper that established the viability of large-scale multi-task robot learning with Transformers. It became the foundation on which RT-2, RT-X, and the entire VLA lineage was built. The robotics community appreciated the scale of real-world data collection, though many noted the Google-specific infrastructure required. The paper catalyzed a wave of efforts in open-source robot datasets (Open X-Embodiment) precisely because the community recognized that RT-1's data advantage was not reproducible outside Google. Some researchers criticized the reliance on a single embodiment and controlled lab conditions as limiting the generality of the conclusions.

OPEN QUESTIONS:
How can the data collection bottleneck be addressed without Google-scale resources? Can RT-1-style architectures scale to bimanual or multi-embodiment settings? What is the right trade-off between model capacity and data diversity for robotic control? How should action tokenization be designed to preserve precision while remaining compatible with Transformer architectures? Can sim-to-real transfer reduce the need for real demonstrations?""",

    2: """STRENGTHS:
RT-2 introduced the Vision-Language-Action (VLA) paradigm that defined the field's trajectory. The key insight -- representing robot actions as text tokens and co-fine-tuning a pretrained VLM on web and robot data -- was elegant and opened the door to transferring internet-scale knowledge to robotics. The emergent capabilities demonstrated (reasoning about novel objects, following complex instructions involving unseen concepts) were genuinely impressive and showed that VLMs could bring commonsense knowledge to robotic control. The co-training strategy that mixes web data with robot data to prevent catastrophic forgetting was practical and well-validated. Testing at both 5B and 55B parameter scales provided useful scaling insights.

LIMITATIONS:
The computational cost is extreme: running a 55B parameter model for real-time robot control is impractical outside of Google's infrastructure. Inference latency is a serious concern, with the model requiring significant compute for each action prediction. The paper demonstrates capabilities primarily on the same Everyday Robots hardware as RT-1, so cross-embodiment generalization remains unproven. Action tokenization into discrete text tokens introduces quantization error that limits precision for fine manipulation. The emergent capabilities, while impressive in demos, were tested on a relatively small set of scenarios without rigorous statistical analysis. The reliance on proprietary VLM backbones (PaLI-X, PaLM-E) made the work impossible to reproduce outside Google. The paper conflates two confounding factors -- scale and web knowledge -- making it hard to attribute gains precisely.

COMMUNITY RECEPTION:
RT-2 was one of the most influential robotics papers of 2023. It established the VLA as a new paradigm and directly inspired OpenVLA, Octo, and a wave of open-source alternatives. The community was excited about the vision of leveraging web-scale pretraining for robotics but frustrated by the lack of reproducibility due to proprietary models and Google-internal data. The paper sparked significant debate about whether brute-force scaling of VLMs is the right path for robotics, or whether more sample-efficient and structured approaches are needed. The emergent capability results were celebrated but also met with some skepticism about cherrypicking.

OPEN QUESTIONS:
Can VLA-style transfer be achieved with smaller, open-source models without catastrophic loss of emergent capabilities? What is the minimum model scale needed for meaningful semantic transfer to robotics? How should action spaces be represented to avoid the precision limitations of text tokenization? Is co-training the right paradigm, or should VLM knowledge be distilled into smaller control-specific models? How can VLAs be extended to contact-rich manipulation where language descriptions are insufficient?""",

    3: """STRENGTHS:
Octo was a critical contribution as the first serious open-source generalist robot policy. Training on 800k trajectories from the Open X-Embodiment dataset across multiple robot embodiments, it showed that cross-embodiment pretraining is feasible and provides a useful initialization for downstream fine-tuning. The modular architecture with a Transformer backbone and separate readout heads for different action spaces was well-designed for flexibility. The fine-tuning recipe -- adapting Octo to new robots with as few as 100 demonstrations -- was practical and well-documented. The open release of model weights, code, and training infrastructure was a genuine service to the community.

LIMITATIONS:
Octo's zero-shot transfer performance is weak; it essentially requires fine-tuning for each new task and embodiment, which undermines the "generalist" framing. The base model capacity (93M parameters) is modest compared to VLAs like RT-2, limiting its ability to perform complex reasoning. The diffusion action head, while flexible, adds inference latency and can be unstable during training. Performance across the Open X-Embodiment datasets is uneven, with the model performing well on some embodiments and poorly on others due to data imbalance. The Transformer-only architecture without a pretrained vision-language backbone means Octo lacks the semantic grounding that VLAs get for free. Language conditioning is relatively shallow, and Octo struggles with complex or compositional instructions.

COMMUNITY RECEPTION:
Octo was warmly received as a practical, open alternative to Google's closed models. It became a standard baseline and fine-tuning starting point in many academic labs. The community valued the engineering effort and the open release, though researchers quickly recognized that Octo's zero-shot capabilities were limited compared to VLAs. Octo's influence was more as infrastructure (a reusable pretrained backbone) than as a standalone policy. It was largely superseded by OpenVLA and later models that combined VLM pretraining with robotic fine-tuning, though the fine-tuning paradigm it established remained influential.

OPEN QUESTIONS:
How much does cross-embodiment pretraining actually help versus training from scratch on the target embodiment? What is the optimal architecture for a generalist robot policy -- should it be built on a VLM backbone or trained from scratch on robot data? Can Octo-style models achieve meaningful zero-shot transfer, or is fine-tuning always necessary? How should heterogeneous action spaces across embodiments be unified?""",

    4: """STRENGTHS:
OpenVLA democratized the VLA paradigm by building on Llama 2 7B and releasing everything -- weights, code, data pipeline -- to the community. The key architectural choice of fine-tuning a pretrained VLM (Prismatic VLM) end-to-end on robot data was straightforward and effective. OpenVLA showed that the RT-2 recipe could be replicated with open-source components and that the resulting model outperformed RT-2-X and Octo on standard benchmarks. The training on Open X-Embodiment data demonstrated that VLAs can work across embodiments with a single set of weights. The paper's ablations on VLM backbone choice, data mixtures, and fine-tuning strategies provided practical guidance for the community.

LIMITATIONS:
At 7B parameters, OpenVLA is expensive to deploy on real robots -- inference requires a high-end GPU and runs at limited frequency, creating latency issues for reactive control. The discrete action tokenization (256 bins per dimension) introduces quantization error that is particularly problematic for precise manipulation tasks. The model struggles with tasks requiring fine-grained spatial precision, as the action representation is fundamentally lossy. Single-image input limits temporal reasoning; the model has no mechanism for handling sequential observations or maintaining state. Performance on dexterous manipulation and contact-rich tasks is poor. The model's reliance on the Llama 2 backbone means it inherits the inefficiencies of autoregressive language model inference for what is fundamentally a continuous control problem.

COMMUNITY RECEPTION:
OpenVLA became the de facto open-source VLA baseline and one of the most cited robotics papers of 2024. Its release galvanized a wave of research building on and improving the VLA paradigm. The community appreciated the openness but quickly identified the action tokenization bottleneck and deployment cost as primary issues to solve. OpenVLA spawned a cottage industry of improvements: FAST tokenization, diffusion action heads, chain-of-thought reasoning, and various efficiency optimizations. Some researchers used it as evidence that the VLA paradigm was viable at academic scale, while others saw its limitations as motivation for alternative architectures.

OPEN QUESTIONS:
Can the action tokenization bottleneck be solved without abandoning the VLM backbone? Is 7B the right model size, or can distillation produce smaller VLAs without losing semantic capabilities? How should VLAs handle temporal information -- is single-frame input fundamentally limiting? What is the best way to fine-tune VLAs for specific tasks without catastrophic forgetting of general capabilities?""",

    5: """STRENGTHS:
Pi-Zero introduced flow matching as the action decoding mechanism for VLAs, replacing discrete tokenization with a continuous generative process that naturally handles multimodal action distributions. This was a principled solution to one of the core limitations of autoregressive VLAs. The architecture cleanly separates the VLM backbone (for understanding) from the flow-matching action head (for control), allowing each component to do what it does best. The results on dexterous manipulation tasks -- particularly laundry folding and complex bimanual operations -- were state-of-the-art and demonstrated capabilities beyond what prior VLAs could achieve. The use of a 3B parameter PaLI-Gemma backbone showed that strong VLA performance does not require the largest models. Training on a diverse mixture of single-arm, bimanual, and dexterous tasks showed impressive multi-task scaling.

LIMITATIONS:
Pi-Zero is not open-source, and Physical Intelligence has released limited details about the training data, exact architecture, and training procedure, making it difficult to reproduce or build upon. The flow matching action head, while theoretically elegant, requires iterative denoising at inference time, which adds latency compared to single-step autoregressive decoding. The model was demonstrated primarily on Physical Intelligence's own hardware, and cross-embodiment generalization results are limited. The paper focuses on short-horizon tasks, and it is unclear how well Pi-Zero handles long-horizon planning or tasks requiring extended reasoning. The compute requirements for training are substantial and not publicly disclosed.

COMMUNITY RECEPTION:
Pi-Zero generated significant excitement as a demonstration that VLAs could achieve dexterous manipulation capabilities previously out of reach. The flow matching approach was seen as a compelling alternative to discrete action tokenization. However, the closed nature of the work drew criticism from the academic community -- many viewed it as a strong result that could not be built upon. The robotics research community was divided between admiration for the technical results and frustration with the lack of reproducibility. Pi-Zero is frequently cited as evidence that the VLA paradigm can scale to complex manipulation, but its influence on follow-up research has been limited by the lack of open artifacts.

OPEN QUESTIONS:
Can flow matching VLAs be reproduced at academic scale with open-source components? What is the optimal number of denoising steps at inference for the best latency-quality trade-off? How does Pi-Zero compare to diffusion-based approaches on the same benchmarks? Can the architecture be extended to handle truly long-horizon tasks with intermediate subgoal reasoning?""",

    6: """STRENGTHS:
Pi-Zero.5 extends Pi-Zero with open-world generalization capabilities, showing that a VLA can operate in unstructured, novel environments rather than just controlled lab settings. The model demonstrates impressive zero-shot generalization to new scenes, objects, and task configurations not seen during training. The use of large-scale and diverse training data enables robustness to visual domain shift. The results on household tasks in real, cluttered environments represent a meaningful step toward practical deployment. The hierarchical reasoning capabilities -- combining high-level language understanding with low-level control -- are more sophisticated than prior VLAs.

LIMITATIONS:
Like Pi-Zero, Pi-Zero.5 remains closed-source, severely limiting its impact on academic research. The paper's claims about open-world generalization are difficult to verify independently. The definition of "open-world" is somewhat generous -- the model is still limited to manipulation tasks within a specific action space and embodiment class. Failure modes in truly novel scenarios are not well characterized. The computational requirements are likely even higher than Pi-Zero given the larger training data and model capacity. The evaluation methodology for open-world generalization is inherently subjective and lacks standardized benchmarks.

COMMUNITY RECEPTION:
Pi-Zero.5 was received with interest but also a degree of fatigue with Physical Intelligence's closed-source approach. The robotics community acknowledged the impressive demos but noted the difficulty of distinguishing genuine generalization from carefully selected demonstrations. Comparisons to open-source alternatives like OpenVLA are complicated by differences in evaluation protocols. The paper's influence has been primarily as a north star for what VLAs might achieve rather than as a practical starting point for research. Some researchers have questioned whether the "open-world" framing sets unrealistic expectations for current VLA capabilities.

OPEN QUESTIONS:
What constitutes genuine open-world generalization versus interpolation within a large training distribution? How robust is Pi-Zero.5 to truly out-of-distribution scenarios? Can these capabilities be achieved with open-source models and publicly available data? What safety guarantees can be provided for VLAs deployed in unstructured environments?""",

    7: """STRENGTHS:
FAST (Fast Action Sequence Tokenization) addresses one of the most practical bottlenecks in VLAs: the inefficiency of tokenizing continuous actions as text. By learning a discrete codebook via VQ-VAE that compresses action sequences (chunks) into far fewer tokens than naive per-dimension discretization, FAST dramatically reduces the sequence length the autoregressive model must generate. This speeds up inference by 5-10x in practice and improves action representation quality since the learned codebook captures the structure of robot action distributions. The approach is model-agnostic and can be applied to any autoregressive VLA. The empirical results show improved performance alongside the efficiency gains, which is a rare win-win.

LIMITATIONS:
The VQ-VAE codebook must be trained on action data from the target domain, introducing a data-dependent preprocessing step that complicates the pipeline. Codebook collapse (where only a subset of codes are used) is a known issue with VQ-VAE and requires careful hyperparameter tuning. The compression introduces a representation bottleneck -- unusual or out-of-distribution actions may not be well captured by the learned codebook. The approach has been primarily validated on action tokenization for manipulation; its applicability to other domains (locomotion, driving) is not established. The interaction between codebook quality and downstream VLA performance is not fully understood, and suboptimal codebook training can silently degrade policy quality.

COMMUNITY RECEPTION:
FAST was quickly recognized as an important practical contribution and adopted by several groups building on OpenVLA and similar architectures. The insight that action tokenization is a critical bottleneck resonated widely. The approach was seen as complementary to other VLA improvements (better backbones, diffusion heads) and was adopted in Pi-Zero-FAST and other models. Some researchers noted that FAST treats the symptom (inefficient tokenization) rather than the disease (using autoregressive language model decoders for continuous control), but the practical benefits were hard to argue with.

OPEN QUESTIONS:
What is the optimal codebook size and chunk length for different tasks and embodiments? Can the codebook be learned jointly with the VLA rather than as a separate preprocessing step? How sensitive is VLA performance to codebook quality, and can robustness to codebook errors be improved? Is FAST-style tokenization still needed when the action head uses diffusion or flow matching instead of autoregressive decoding?""",

    8: """STRENGTHS:
This paper tackles a genuine and underappreciated problem: when VLAs are fine-tuned on robot data, they can lose (catastrophically forget) the rich visual and linguistic knowledge from their VLM pretraining. The knowledge insulation approach proposes architectural or training strategies to preserve the VLM's pretrained representations while still learning effective control policies. The paper provides clear empirical evidence that naive fine-tuning degrades visual understanding and language following capabilities, and that insulation techniques mitigate this. The analysis of which layers and representations are most vulnerable to forgetting is insightful and actionable.

LIMITATIONS:
The insulation strategies often involve trade-offs: freezing too many VLM layers preserves knowledge but limits the model's ability to adapt to the robot domain. The paper does not fully resolve this tension. The evaluation of "knowledge preservation" relies on proxy metrics (performance on VLM benchmarks) that may not capture the specific types of knowledge most useful for robotics. The approach adds complexity to the training pipeline and requires careful tuning of which components to freeze or regularize. The experimental scope is limited to a few VLA architectures, and it is unclear how well the findings generalize to larger models or different pretraining recipes.

COMMUNITY RECEPTION:
The paper was well-received as an important diagnostic contribution that named and quantified a problem many practitioners had observed informally. It influenced how subsequent VLA papers approached fine-tuning, with many adopting partial freezing or regularization strategies. However, the specific insulation techniques proposed were seen as incremental rather than transformative. The community recognized the problem as fundamental but debated whether insulation is the right solution versus alternative approaches like adapter-based fine-tuning, prompt tuning, or co-training on mixed data.

OPEN QUESTIONS:
What is the optimal balance between knowledge preservation and task adaptation for VLAs? Can adapter or LoRA-based approaches achieve insulation more cleanly than full fine-tuning with regularization? How much VLM knowledge is actually useful for robotic control, and can this be quantified? Does the catastrophic forgetting problem diminish as models scale up?""",

    9: """STRENGTHS:
Diffusion Policy was a genuinely transformative contribution that introduced denoising diffusion models as a principled mechanism for learning robot visuomotor policies. The key insight -- that diffusion models can naturally represent multimodal action distributions, which are ubiquitous in manipulation due to multiple valid strategies for any given task -- was both theoretically motivated and practically impactful. The action chunk prediction approach (predicting a sequence of future actions) provides temporal consistency that frame-by-frame policies lack. The paper provides thorough analysis of both CNN-based and Transformer-based diffusion architectures, with clear trade-offs documented. Evaluation across multiple simulation and real-world tasks demonstrated consistent improvements over prior methods like IBC and BET.

LIMITATIONS:
The primary limitation is inference latency: iterative denoising requires 10-100 forward passes to generate a single action chunk, creating a fundamental tension with real-time control requirements. While DDIM acceleration helps, the latency is still significantly higher than single-pass models. Training can be unstable with high-dimensional action spaces, requiring careful noise schedule tuning. The diffusion process operates on action space directly, which may not be the right abstraction for tasks where the optimal action distribution has complex structure. Diffusion Policy requires demonstration data with full state-action trajectories, and its performance degrades significantly with noisy or suboptimal demonstrations. The method does not incorporate language conditioning in its original form, limiting its use in instruction-following settings.

COMMUNITY RECEPTION:
Diffusion Policy became one of the most influential papers in robot learning, spawning an entire subfield of diffusion-based robot policies. It was rapidly adopted as a standard baseline and building block, with follow-up work on 3D diffusion (DP3), diffusion for VLAs (DiffusionVLA, LLaDA-VLA), and accelerated inference. The paper won multiple awards and was recognized for establishing a new paradigm. The community debate centered on whether diffusion's benefits (multimodality, expressiveness) outweigh its costs (latency, complexity) compared to simpler alternatives like action chunking transformers. Some researchers argued the multimodality benefit is overstated for most practical tasks.

OPEN QUESTIONS:
Can diffusion policy inference be made truly real-time (100+ Hz) without sacrificing quality? Is multimodal action representation actually necessary for most manipulation tasks, or is it solving a theoretical rather than practical problem? How should diffusion policies be combined with high-level language or goal conditioning? Can consistency models or flow matching provide the same benefits with fewer denoising steps?""",

    10: """STRENGTHS:
ACT (Action Chunking with Transformers) demonstrated that a simple CVAE (Conditional Variational Autoencoder) architecture with action chunking can achieve remarkably effective fine-grained bimanual manipulation. The simplicity of the approach is a strength -- compared to the complexity of diffusion or VLA methods, ACT uses a straightforward encoder-decoder Transformer with a learned prior. The action chunking mechanism (predicting sequences of future actions) combined with temporal ensembling produces smooth, temporally consistent trajectories. The ALOHA hardware platform, co-released with the paper, became a standard platform for bimanual manipulation research. The demonstration that teleoperated demonstrations with action chunking can learn precise tasks like inserting a battery or threading a zip tie was impressive.

LIMITATIONS:
ACT requires high-quality teleoperation demonstrations and is sensitive to demonstration quality -- noisy or inconsistent demos lead to poor policies. The CVAE formulation, while enabling multimodal action prediction in principle, often collapses to a single mode in practice, limiting its diversity. The model does not incorporate language conditioning, making it a per-task policy rather than a generalizable multi-task model. Generalization to new objects or configurations is limited; ACT policies tend to be brittle outside the demonstration distribution. The model has no mechanism for error recovery or replanning. The temporal ensembling heuristic, while effective, is ad hoc and can introduce lag in reactive control.

COMMUNITY RECEPTION:
ACT was enormously influential, particularly because the co-released ALOHA hardware made bimanual manipulation accessible to academic labs. The combination of simple algorithm and practical hardware enabled rapid adoption. Many labs built ALOHA platforms and used ACT as a starting point, leading to variants like Mobile ALOHA. The paper demonstrated that sophisticated manipulation is achievable without VLM-scale models, which was a useful counterpoint to the scaling narrative. However, ACT's per-task nature and limited generalization meant it was viewed more as a practical tool than a path to general-purpose manipulation. The community debated whether ACT's simplicity was a feature or a limitation relative to diffusion-based approaches.

OPEN QUESTIONS:
Can ACT-style architectures be extended to multi-task and language-conditioned settings without losing their simplicity? Is the CVAE formulation the right choice, or would other latent variable models work better? How can ACT policies be made more robust to distribution shift and demonstration noise? What is the minimum number of demonstrations needed for reliable ACT policies on complex bimanual tasks?""",

    11: """STRENGTHS:
HybridVLA addresses a real architectural tension in VLAs: autoregressive decoding is natural for language understanding but suboptimal for continuous action generation, while diffusion excels at actions but cannot handle discrete language. By combining autoregressive processing for vision-language understanding with a diffusion head for action generation, HybridVLA gets the best of both worlds. The architecture allows the VLM backbone to focus on perception and reasoning while the diffusion head handles the continuous control problem. Empirical results show improvements over both pure autoregressive VLAs (like OpenVLA) and standalone diffusion policies, particularly on tasks requiring both semantic understanding and precise control.

LIMITATIONS:
The hybrid architecture introduces additional complexity in both training and inference. The interface between the autoregressive backbone and the diffusion head requires careful design, and the paper does not fully explore how information flows between these components. Training requires balancing two different loss functions (language modeling and diffusion denoising) which can be tricky to tune. The diffusion head still incurs iterative denoising cost at inference. The evaluation is primarily on standard benchmarks and does not demonstrate clear advantages on tasks that specifically require both strong language understanding and precise continuous control. The approach has not been validated at large scale or across diverse embodiments.

COMMUNITY RECEPTION:
HybridVLA was seen as a sensible architectural evolution that formalized what many researchers had been experimenting with informally. The idea of using the right inductive bias for each component resonated with the community. However, some researchers questioned whether the added complexity is justified relative to the gains, especially given that simpler approaches (like FAST tokenization for autoregressive models) can partially address the same issues. The paper influenced the design of subsequent VLAs including DiffusionVLA and LLaDA-VLA, which explored similar hybrid strategies.

OPEN QUESTIONS:
What is the optimal interface between the language model backbone and the diffusion action head? Can the diffusion head be replaced with flow matching or consistency models for faster inference? How does the hybrid approach scale with model size -- does the benefit of separation increase or decrease? Is there a principled way to decide which components should be autoregressive versus diffusion-based?""",

    12: """STRENGTHS:
DiffusionVLA (DiVLA) provides a systematic investigation of how to scale diffusion-based action decoding within VLA architectures. The paper argues that diffusion action heads, when properly integrated with VLM backbones, can serve as robot foundation models with better scaling properties than autoregressive action decoding. The scaling analysis -- examining how performance improves with model size, data quantity, and diffusion head capacity -- is valuable for the community's understanding of VLA scaling laws. The results show competitive or superior performance to autoregressive VLAs on manipulation benchmarks while maintaining the continuous action representation advantages of diffusion.

LIMITATIONS:
The scaling analysis, while useful, is limited to a relatively narrow range of model sizes and data quantities compared to what would be needed for true scaling law analysis. The paper does not provide a clear theoretical justification for why diffusion action heads should scale differently than autoregressive heads. Inference cost scales with both the VLM backbone and the diffusion denoising steps, creating a compounded latency problem. The evaluation focuses on standard manipulation benchmarks that may not reveal the unique advantages of the diffusion approach. The training infrastructure requirements are substantial, and the paper does not adequately address how to make this approach accessible to labs with limited compute.

COMMUNITY RECEPTION:
DiVLA was seen as a useful data point in the ongoing debate between autoregressive and diffusion-based action decoding for VLAs. The scaling analysis was appreciated, though some researchers felt the conclusions were preliminary given the limited scale of experiments. The paper contributed to a growing consensus that diffusion action heads are a viable alternative to tokenized actions, but it did not definitively resolve the question of which approach is superior. The work was cited frequently in the context of VLA architecture design discussions.

OPEN QUESTIONS:
Do diffusion-based VLAs exhibit fundamentally different scaling behavior than autoregressive VLAs, or do the differences diminish at scale? What is the optimal ratio of compute allocated to the VLM backbone versus the diffusion head? Can the diffusion head be distilled into a single-step model after training to eliminate the inference latency penalty? How do diffusion VLAs compare on tasks specifically designed to test multimodal action distribution modeling?""",

    13: """STRENGTHS:
Discrete Diffusion VLA introduces an innovative approach by applying discrete diffusion (diffusion in discrete token space rather than continuous space) to action decoding in VLAs. This is a clever compromise: it maintains compatibility with the discrete token framework of language models while gaining the iterative refinement benefits of diffusion. The approach avoids the quantization artifacts of standard action tokenization since the discrete diffusion process can correct errors through iterative denoising. The method is architecturally simpler to integrate with existing VLM backbones than continuous diffusion heads, since everything operates in the same token space.

LIMITATIONS:
Discrete diffusion is a relatively nascent technique, and its theoretical properties for control problems are not as well understood as continuous diffusion. The number of discrete denoising steps still adds inference latency, partially negating the simplicity benefits. The approach inherits the granularity limitations of the discrete action vocabulary -- it cannot represent actions at finer resolution than the vocabulary allows. The empirical comparison with continuous diffusion approaches is limited, making it hard to assess whether discrete diffusion offers genuine advantages or just different trade-offs. The interaction between the discrete diffusion process and the pretrained language model's token distributions is complex and not fully analyzed.

COMMUNITY RECEPTION:
The paper was received with intellectual curiosity as a novel combination of ideas from the NLP and generative modeling communities. Researchers appreciated the creative approach to bridging discrete language models with continuous control. However, it was seen more as an interesting exploration than a definitive solution. The discrete diffusion approach has not been widely adopted, partly because continuous diffusion heads and FAST-style tokenization have been perceived as more mature solutions to the same problem. The paper's influence has been primarily in expanding the design space of VLA action decoders.

OPEN QUESTIONS:
Under what conditions does discrete diffusion outperform continuous diffusion or autoregressive decoding for action generation? Can discrete diffusion leverage the pretrained language model's distribution in a meaningful way, or does it need to learn action distributions from scratch? How does the approach scale with vocabulary size and action dimensionality? Is there a principled way to combine discrete diffusion for actions with standard autoregressive generation for language?""",

    14: """STRENGTHS:
Dita brings the Diffusion Transformer (DiT) architecture -- which has been highly successful in image generation -- to the VLA setting, replacing the U-Net-based diffusion action heads with a Transformer-based diffusion model. This allows better scaling behavior and more natural integration with the Transformer-based VLM backbone. The paper demonstrates that the DiT architecture for action denoising achieves stronger performance than U-Net alternatives, particularly as model size increases. The approach supports flexible conditioning mechanisms through cross-attention with VLM features. The scaling experiments show encouraging trends that action quality improves consistently with DiT model size.

LIMITATIONS:
The Transformer-based diffusion head is computationally expensive, combining the cost of the VLM backbone with a large Transformer for denoising. Memory requirements during training are substantial due to the need to backpropagate through both the VLM and the DiT. The paper does not provide clear inference latency comparisons against simpler alternatives at the same accuracy level. The DiT architecture introduces additional hyperparameters (number of layers, attention heads, conditioning mechanism) that require tuning for each domain. The scaling analysis, while promising, covers a limited range and does not establish whether the observed trends continue at much larger scales.

COMMUNITY RECEPTION:
Dita was recognized as a natural and well-executed adaptation of the DiT architecture to robotics. The community saw it as part of a broader trend of importing successful generative modeling architectures from vision to robotics. The paper strengthened the case for Transformer-based architectures throughout the VLA pipeline. However, some researchers questioned whether the architectural improvements in the diffusion head matter as much as improvements in the VLM backbone or training data. The work influenced subsequent VLA designs that adopted Transformer-based action heads.

OPEN QUESTIONS:
Where is the performance bottleneck in VLAs: the perception backbone, the language understanding, or the action generation? Does the DiT action head benefit from scaling as much as DiT does for image generation? Can the DiT and VLM backbone share parameters or computation to reduce the total inference cost? How do DiT-based action heads compare with flow matching alternatives on the same benchmarks?""",

    15: """STRENGTHS:
CogACT introduces cognitive-inspired mechanisms for action prediction in VLAs, proposing that intermediate reasoning or cognitive steps before action generation can improve policy quality. The paper draws meaningful analogies between human cognitive processes in motor control and computational mechanisms in VLAs. The proposed architecture explicitly models the deliberation process between perceiving a scene and generating actions, adding interpretability to the VLA pipeline. Results show improvements on tasks that require understanding spatial relationships, physical properties, or multi-step reasoning about manipulation strategies.

LIMITATIONS:
The cognitive framing, while intuitive, is somewhat loosely connected to the actual architectural contributions. The "cognitive" steps could be interpreted as simply adding more compute or intermediate representations, and the paper does not rigorously distinguish between the cognitive hypothesis and simpler explanations for the performance gains. The additional inference cost of the cognitive processing steps adds latency. The evaluation does not include sufficiently diverse baselines to determine whether the gains come from the cognitive architecture specifically or from the increased model capacity. The connection to actual cognitive science literature is surface-level.

COMMUNITY RECEPTION:
CogACT was received with moderate interest. The community appreciated the attempt to add interpretable reasoning to VLAs but was divided on whether the cognitive framing adds genuine insight versus being a rebranding of standard intermediate representation techniques. The paper contributed to the growing interest in chain-of-thought and reasoning mechanisms for robotic control (alongside ECoT and CoT-VLA), but its specific cognitive mechanisms were not widely adopted. Researchers noted the overlap with concurrent work on chain-of-thought for VLAs and questioned the novelty of the specific approach.

OPEN QUESTIONS:
What types of tasks genuinely benefit from explicit cognitive reasoning steps versus direct perception-to-action mapping? Can the cognitive steps be made more interpretable, for instance by producing human-readable reasoning traces? How does CogACT's approach relate to and compare with chain-of-thought methods like ECoT and CoT-VLA? Is there a principled way to determine how much "thinking" is needed for a given task?""",

    16: """STRENGTHS:
RDT-1B is the first billion-parameter diffusion foundation model specifically designed for bimanual manipulation, addressing a gap left by most VLAs that focus on single-arm tasks. The model's capacity at 1.1B parameters allows it to absorb diverse bimanual manipulation data and generalize across tasks. The diffusion-based action generation is well-suited for bimanual control where coordinating two arms creates inherently multimodal action distributions. The paper demonstrates effective few-shot adaptation to new bimanual tasks, which is practically valuable given the difficulty of collecting bimanual demonstrations. The model handles the 14-DoF action space (7 per arm) gracefully.

LIMITATIONS:
The 1B parameter scale, while enabling capacity, creates deployment challenges for real-time bimanual control where both arms need coordinated low-latency commands. The diffusion inference process compounds this latency issue. The training data for bimanual manipulation is much more limited than for single-arm tasks, and the paper does not fully address how to handle this data scarcity. The model lacks language conditioning, limiting it to goal-image or task-embedding conditioning rather than natural language instructions. Generalization across different bimanual robot platforms (different arm configurations, workspaces) is not demonstrated. The coordination strategies between arms are learned implicitly rather than explicitly modeled.

COMMUNITY RECEPTION:
RDT-1B was well-received as a needed contribution to the underserved area of bimanual manipulation learning. The community recognized the practical importance of bimanual capabilities and appreciated the scale of the model. The paper was particularly influential among labs building ALOHA-style bimanual platforms. However, some researchers noted that applying diffusion at this scale to bimanual control was a straightforward extension of existing ideas rather than a novel architectural contribution. The work motivated further investigation into foundation models for multi-arm systems.

OPEN QUESTIONS:
How should bimanual coordination be modeled -- implicitly through joint action prediction or explicitly through coordination mechanisms? Can RDT-1B-style models be combined with VLM backbones for language-conditioned bimanual manipulation? What is the minimum data requirement for effective bimanual foundation models? How do bimanual diffusion policies handle the combinatorial complexity of two-arm contact states?""",

    17: """STRENGTHS:
LLaDA-VLA integrates diffusion action generation directly into a VLA framework built on a language model backbone, creating a unified model that can process language, reason about scenes, and generate continuous actions through diffusion. The approach addresses the key weakness of autoregressive VLAs (discretized action spaces) while maintaining the language understanding capabilities of the VLM backbone. The unified training objective that combines language modeling with diffusion denoising is technically interesting. The paper demonstrates that the diffusion components do not significantly degrade language understanding performance while substantially improving action quality.

LIMITATIONS:
The training procedure is complex, requiring careful balancing of language modeling and diffusion losses, with the risk that one objective dominates the other. The architecture requires modifications to the standard language model to accommodate the diffusion components, making it incompatible with off-the-shelf VLMs and requiring training from scratch or significant adaptation. Inference involves both autoregressive language generation and iterative diffusion denoising, creating a complex and potentially slow pipeline. The paper does not provide detailed latency analysis comparing against simpler alternatives. The evaluation is limited to standard manipulation benchmarks and does not test the language understanding capabilities rigorously.

COMMUNITY RECEPTION:
LLaDA-VLA was seen as a natural evolution in the design space of VLAs, combining the strengths of the diffusion policy and VLA paradigms. The community appreciated the technical integration but noted the engineering complexity involved. The paper contributed to the growing consensus that hybrid autoregressive-diffusion architectures are promising for VLAs. Some researchers expressed concern about the training complexity and questioned whether the unified approach is worth the cost compared to modular designs that combine a standard VLM with a separate diffusion policy head.

OPEN QUESTIONS:
Is it better to train a unified language-diffusion model or to combine separately trained components? How should the training losses be balanced to ensure both language understanding and action quality? Can the diffusion components be added to an existing pretrained VLM through efficient fine-tuning rather than training from scratch? What is the optimal architecture for the diffusion components within a language model framework?""",

    18: """STRENGTHS:
ECoT (Embodied Chain-of-Thought) was a pioneering contribution that brought chain-of-thought reasoning to VLAs, showing that having the model verbalize its reasoning process (identifying objects, planning steps, describing spatial relationships) before generating actions improves performance on complex tasks. The paper demonstrates that this reasoning capability emerges naturally from fine-tuning with chain-of-thought annotations, and that the reasoning traces are often sensible and interpretable. Performance improvements are most pronounced on tasks requiring multi-step reasoning, spatial understanding, or handling of novel objects. The approach leverages the VLM's existing language generation capabilities in a natural way.

LIMITATIONS:
Generating chain-of-thought reasoning tokens before actions significantly increases inference latency, which is problematic for real-time robot control. The quality of reasoning is limited by the chain-of-thought annotations in the training data, creating a data curation bottleneck. The reasoning traces, while often plausible, can be confabulations that do not reflect the model's actual decision-making process -- the correlation between reasoning quality and action quality is not perfect. The approach works best with large VLM backbones that have strong language generation capabilities, limiting its applicability to smaller models. The chain-of-thought annotations require significant human effort to create.

COMMUNITY RECEPTION:
ECoT was influential in establishing the idea that VLAs should "think before acting," which became a major research direction. The paper inspired follow-up work including CoT-VLA, CoA-VLA, and MoTVLA. The community found the interpretability benefits compelling -- being able to read what the robot is "thinking" is valuable for debugging and trust-building. However, there was debate about whether the reasoning is genuinely causal (does it improve actions?) or merely correlated (do better models produce both better reasoning and better actions?). The latency cost was recognized as a significant practical limitation for deployment.

OPEN QUESTIONS:
Is chain-of-thought reasoning genuinely causal for improving robotic actions, or is it an epiphenomenon of model quality? Can the reasoning be compressed into learned representations (implicit chain-of-thought) to avoid the latency cost of text generation? What is the right granularity for chain-of-thought in robotics -- high-level plans, spatial descriptions, or something else? How should chain-of-thought annotations be generated at scale?""",

    19: """STRENGTHS:
CoA-VLA (Chain-of-Affordance) introduces affordance reasoning as a structured intermediate representation between perception and action in VLAs. Rather than generic chain-of-thought, the model specifically reasons about where to grasp, which surfaces afford manipulation, and what contact points are relevant. This is a more robotics-specific form of reasoning than generic language-based chain-of-thought. The affordance predictions can be visualized and verified, providing interpretability. The approach grounds the VLA's reasoning in physical properties of the scene rather than abstract language descriptions. Results show improvements specifically on tasks where affordance reasoning is critical, such as grasping novel objects.

LIMITATIONS:
The affordance-specific reasoning pipeline requires affordance annotations in the training data, which are expensive to collect and may not be available for many robotic domains. The definition of "affordance" used in the paper is relatively narrow (primarily grasp affordances) and does not capture the full richness of affordance theory from ecological psychology. The visual-text affordance chain adds computational cost and may be unnecessary for simple tasks where direct perception-to-action mapping suffices. The approach is evaluated primarily on grasping-centric tasks, and its benefits for other manipulation skills (pushing, inserting, tool use) are not demonstrated. The integration of affordance reasoning with the VLM backbone requires architectural modifications.

COMMUNITY RECEPTION:
CoA-VLA was received positively as a more structured alternative to generic chain-of-thought reasoning for VLAs. The community appreciated the connection to affordance theory and the focus on physically grounded reasoning. However, some researchers noted that predicting affordances is itself a hard problem and questioned whether adding this intermediate step truly simplifies the overall learning problem. The work was compared favorably to ECoT for its specificity but noted as less general. The affordance prediction component was seen as potentially useful as a standalone module independent of the VLA framework.

OPEN QUESTIONS:
Can affordance reasoning be extended beyond grasping to encompass a broader set of manipulation primitives? How should affordance annotations be generated at scale -- can they be automated through simulation or self-supervision? Does affordance-based reasoning compose well with other forms of chain-of-thought (spatial, temporal, causal)? Is explicit affordance prediction necessary, or can it be captured implicitly by sufficiently large VLMs?""",

    20: """STRENGTHS:
CoT-VLA provides a systematic study of visual chain-of-thought reasoning for VLAs, where the model generates visual representations (such as predicted future images, keypoint trajectories, or spatial annotations) as intermediate reasoning steps before action generation. This is a meaningful extension beyond text-only chain-of-thought, since much of robotic reasoning is inherently spatial and visual. The visual chain-of-thought provides intuitive and verifiable intermediate predictions. The paper shows that visual reasoning helps most on tasks with complex spatial configurations, precisely where text-based reasoning is least natural.

LIMITATIONS:
Generating visual predictions as intermediate representations is computationally expensive and adds significant latency to the already slow VLA inference pipeline. The quality of visual predictions (future frames, keypoints) is limited by the model's visual generation capabilities, and errors in visual predictions can propagate to incorrect actions. Training the visual chain-of-thought requires paired data of visual reasoning traces and actions, which is challenging to collect. The approach is inherently limited to the visual representations the model is trained to generate -- it cannot reason about aspects of the scene not captured by the chosen visual representation. The paper does not adequately compare against simpler baselines that achieve similar spatial reasoning through other means.

COMMUNITY RECEPTION:
CoT-VLA was seen as a logical next step after ECoT, extending chain-of-thought from text to visual modalities. The community found the visual reasoning traces compelling and intuitive. However, there was concern about the compounding of computational costs (VLM backbone + visual generation + action generation) and whether the benefits justify this cost. The paper was part of a cluster of concurrent works on reasoning for VLAs (ECoT, CoA-VLA, MoTVLA), and the community debated which form of intermediate reasoning is most effective. Some researchers argued that visual chain-of-thought is trying to solve the wrong problem -- that VLAs need better perception, not explicit visual reasoning.

OPEN QUESTIONS:
What visual representations are most useful for chain-of-thought in robotic manipulation (keypoints, flow fields, depth maps, future frames)? Can visual chain-of-thought be generated in latent space to avoid the cost of pixel-space generation? How should visual and textual chain-of-thought be combined? Does visual chain-of-thought genuinely improve the model's spatial understanding, or does it just add a useful inductive bias during training?""",

    21: """STRENGTHS:
Gemini Robotics represents Google DeepMind's most ambitious integration of their flagship Gemini models with robotic control. The paper demonstrates that state-of-the-art multimodal models (with strong vision, language, and reasoning capabilities) can be adapted for robotic manipulation with impressive results. The scale of the backbone model provides unmatched semantic understanding, enabling complex instruction following, multi-step reasoning about tasks, and generalization to novel scenarios. The integration of Gemini's long-context capabilities allows for richer interaction histories and more sophisticated task specifications. The results on challenging real-world tasks, including those requiring common sense reasoning, represent a new state-of-the-art.

LIMITATIONS:
Gemini Robotics requires Google-scale infrastructure for both training and inference, making it inaccessible to the broader research community. The proprietary nature of both the Gemini backbone and the robotics-specific fine-tuning data limits reproducibility. The inference latency of running Gemini-scale models for real-time control is not adequately addressed. The paper focuses on relatively constrained manipulation scenarios despite the model's reasoning capabilities, raising questions about whether the massive computational cost is justified for the demonstrated tasks. The evaluation methodology is not standardized against community benchmarks, making comparisons with other VLAs difficult. Safety and reliability for such powerful but potentially unpredictable systems are not thoroughly discussed.

COMMUNITY RECEPTION:
Gemini Robotics generated significant attention as a demonstration of what is possible when cutting-edge foundation models are applied to robotics. The community was impressed by the capabilities but frustrated by the complete lack of open-source artifacts. The paper intensified the debate about the role of industry scale in robotics research and whether academic labs can meaningfully contribute to VLA development. Some researchers viewed it as further evidence for the scaling hypothesis in robotics, while others argued it represents an unsustainable approach that optimizes for impressive demos over reproducible science. The work also raised questions about Google's commitment to the robotics community given the closure of Everyday Robots.

OPEN QUESTIONS:
Is there a practical path to deploying Gemini-scale models on real robots, or will distillation always be necessary? How much of the performance comes from the Gemini backbone versus the robotics-specific training? Can the capabilities demonstrated be achieved with open-source models at a fraction of the compute? What are the safety implications of deploying such powerful and potentially unpredictable models in physical environments?""",

    22: """STRENGTHS:
MoTVLA (Mixture of Thought VLA) proposes an elegant solution to the latency problem of chain-of-thought VLAs by introducing a fast-slow thinking mechanism. The model can dynamically decide whether a given situation requires slow, deliberate reasoning (chain-of-thought) or fast, reactive control (direct action prediction). This mirrors dual-process theories from cognitive science and is practically motivated: not every timestep requires deep reasoning. The approach reduces average inference latency compared to always-on chain-of-thought while preserving reasoning capabilities when needed. The gating mechanism that decides between fast and slow modes is learned end-to-end.

LIMITATIONS:
The fast-slow gating mechanism introduces a meta-decision that is itself difficult to learn correctly. Incorrect gating -- using fast mode when slow reasoning is needed, or vice versa -- can lead to failures that are hard to diagnose. The training procedure requires examples of both fast and slow reasoning, and the balance between them can be difficult to tune. The paper does not provide a thorough analysis of when the model chooses fast versus slow modes and whether these decisions align with human intuitions. The evaluation does not adequately test failure modes of the gating mechanism. The approach adds architectural complexity without a clear analysis of the marginal benefit over simpler approaches like always using fast inference with periodic slow reasoning.

COMMUNITY RECEPTION:
MoTVLA was seen as a thoughtful contribution to the practical deployment of reasoning-capable VLAs. The fast-slow framing resonated with the community's concern about inference latency. The connection to dual-process theory was appreciated as a useful conceptual framework. However, some researchers questioned whether the complexity of the gating mechanism is justified and suggested simpler heuristics (like reasoning only at decision points or keyframes) might achieve similar benefits. The paper was part of the broader trend toward making VLA reasoning more efficient and practical.

OPEN QUESTIONS:
Can the fast-slow gating mechanism be made more reliable and interpretable? Are there simpler alternatives to learned gating, such as rule-based or uncertainty-based switching? How does the fast-slow trade-off change across different task complexities and time horizons? Can the slow reasoning mode be compressed or cached to reduce its latency when it is needed?""",

    23: """STRENGTHS:
SpatialVLM addresses a critical gap in VLMs: the lack of explicit spatial reasoning capabilities needed for robotic manipulation. The paper shows that standard VLMs trained on web data have poor understanding of 3D spatial relationships, metric distances, and relative positions -- all essential for robotics. By fine-tuning on spatially-annotated data, SpatialVLM achieves strong spatial reasoning while maintaining general VLM capabilities. The approach of generating spatial reasoning data automatically from 3D annotations is practical and scalable. The resulting model can answer questions about distances, sizes, and spatial relationships that standard VLMs fail at, which is directly useful for robotic planning and manipulation.

LIMITATIONS:
The spatial reasoning capabilities, while improved over standard VLMs, are still approximate and can fail on complex 3D scenes or unusual viewpoints. The spatial data generation pipeline relies on existing 3D annotations or depth estimation, which introduces errors that propagate to the trained model. The paper focuses on spatial question-answering rather than demonstrating direct improvements in downstream robotic control, leaving a gap between spatial understanding and action generation. The approach is evaluated primarily on indoor scenes with tabletop objects, and generalization to outdoor or large-scale environments is not tested. The metric spatial predictions (exact distances, sizes) have limited precision.

COMMUNITY RECEPTION:
SpatialVLM was well-received as a clear identification and partial solution of the spatial reasoning gap in VLMs. The paper influenced subsequent work that incorporated spatial understanding into VLAs. Researchers working on robot manipulation appreciated the explicit focus on spatial capabilities that are often taken for granted in the VLM literature. The data generation pipeline was recognized as a practical contribution. However, some researchers noted that the paper addresses perception (understanding spatial relationships) without demonstrating the connection to control (generating actions based on spatial understanding), and questioned whether a separate spatial reasoning module is needed versus building spatial understanding into the VLA end-to-end.

OPEN QUESTIONS:
How should spatial reasoning capabilities be integrated into VLAs for downstream robotic control? Is explicit spatial training necessary, or can spatial understanding emerge from sufficient robot interaction data? How precise do spatial predictions need to be for effective robotic manipulation? Can SpatialVLM-style training be combined with 3D representations (point clouds, NeRFs) for more robust spatial reasoning?""",

    24: """STRENGTHS:
SpatialBot extends spatial reasoning in VLMs by incorporating depth information and multi-view reasoning capabilities. The model takes RGB-D input or multiple views and can reason about 3D spatial relationships more accurately than RGB-only models like SpatialVLM. The inclusion of depth as a first-class input modality is well-motivated for robotic applications where depth sensors are standard. The paper demonstrates improved spatial reasoning on benchmarks requiring depth understanding, 3D object localization, and spatial relationship prediction. The training data pipeline that leverages depth for spatial annotation generation is practical.

LIMITATIONS:
The reliance on depth input limits applicability to scenarios where reliable depth data is available; performance degrades with noisy or missing depth information. The model architecture modifications to handle depth input make it incompatible with standard VLM pretrained weights, requiring specialized pretraining. The paper does not demonstrate significant improvements on downstream robotic tasks compared to systems that extract depth separately and use it as an auxiliary input. Multi-view reasoning, while conceptually appealing, requires multiple camera viewpoints that are not always available in real robot setups. The evaluation benchmarks are focused on spatial QA rather than robotic manipulation performance.

COMMUNITY RECEPTION:
SpatialBot was received as a reasonable extension of spatial reasoning research for VLMs. The community appreciated the inclusion of depth as an input modality, which is natural for robotics. However, the paper was viewed as incremental over SpatialVLM, and some researchers questioned whether modifying the VLM architecture for depth input is the right approach versus using depth through a separate 3D perception module. The work contributed to the broader understanding that standard VLMs need spatial enhancements for robotic applications but did not become a widely adopted model.

OPEN QUESTIONS:
Is it better to build depth understanding into the VLM or to use a separate 3D perception pipeline? How robust is depth-conditioned spatial reasoning to sensor noise and calibration errors? Can the approach be extended to handle arbitrary sensor configurations (different cameras, depth sensors)? What is the right representation for depth information in VLMs -- raw depth maps, point clouds, or learned 3D features?""",

    25: """STRENGTHS:
3D-VLA proposes using explicit 3D representations (point clouds, voxels, or neural fields) as the perceptual backbone for VLAs, rather than relying on 2D images processed by standard VLMs. This is a principled approach since robotic manipulation is fundamentally a 3D problem, and 2D projections lose critical depth, occlusion, and spatial information. The paper demonstrates that 3D-aware VLAs can achieve better spatial precision in action prediction and more robust generalization to viewpoint changes. The integration of 3D representations with language conditioning allows for spatially grounded language understanding.

LIMITATIONS:
3D representations (point clouds, voxels) are computationally expensive to process and do not benefit from the massive pretrained 2D vision encoders available for standard VLMs. This creates a tension between 3D geometric fidelity and the rich semantic features from 2D pretraining. Point cloud processing with Transformers is memory-intensive and scales poorly with scene complexity. The approach requires depth sensors or multi-view 3D reconstruction, adding hardware or computational requirements. The paper does not convincingly demonstrate advantages over simpler approaches that augment 2D VLAs with depth information. The 3D backbone lacks the pretrained semantic understanding of 2D VLMs, requiring more robot-specific training data.

COMMUNITY RECEPTION:
3D-VLA sparked interesting discussion about the role of 3D representations in VLAs. Researchers working on 3D vision and robotics appreciated the push toward geometrically grounded models. However, the mainstream VLA community was somewhat skeptical, noting that 2D VLMs with implicit 3D understanding (from scale and pretraining) often outperform explicit 3D approaches on practical benchmarks. The paper was seen as conceptually important but practically challenging due to the loss of pretrained 2D features. The debate between 2D-pretrained-then-3D-augmented versus 3D-native approaches remains unresolved.

OPEN QUESTIONS:
Can 3D and 2D representations be effectively combined in VLAs to get both geometric precision and semantic richness? How should 3D pretraining be done for robotic VLAs -- on large-scale 3D datasets, through simulation, or through reconstruction? Is there a fundamental advantage to explicit 3D representations for manipulation, or can sufficiently large 2D VLMs learn equivalent spatial understanding? How do 3D VLAs handle partial observations and occlusions?""",

    26: """STRENGTHS:
3D Diffusion Policy (DP3) extends diffusion policy to operate on 3D point cloud observations, combining the expressiveness of diffusion-based action generation with the geometric precision of 3D perception. The paper convincingly demonstrates that 3D observations provide more robust and generalizable policies than 2D image-based alternatives, particularly for tasks involving precise spatial manipulation, viewpoint changes, and clutter. The approach handles varying point cloud densities and partial observations gracefully. Results across multiple simulation benchmarks show consistent improvements over 2D diffusion policy, and the sim-to-real transfer results are compelling. The relatively compact model size makes it practical for real robot deployment.

LIMITATIONS:
Point cloud processing adds computational overhead compared to 2D image encoding, and the combined cost of 3D encoding plus diffusion denoising can be prohibitive for high-frequency control. The approach requires reliable point cloud input, which depends on depth sensor quality and point cloud preprocessing. Performance can degrade with very sparse or noisy point clouds. The method does not incorporate language conditioning, limiting it to goal-conditioned or task-specific policies. DP3 does not leverage pretrained 2D vision features, so it requires more task-specific training data than methods that benefit from ImageNet or CLIP pretraining. Scaling to large, complex scenes with many objects is challenging due to memory constraints of 3D Transformer attention.

COMMUNITY RECEPTION:
DP3 was well-received as a strong contribution that validated the use of 3D representations for diffusion-based robot learning. The community appreciated the thorough experimental evaluation, particularly the viewpoint robustness results that clearly demonstrated the advantages of 3D input. DP3 became a standard baseline for 3D robot learning and influenced subsequent work on 3D-aware policies. The paper was seen as making a compelling case for 3D representations in settings where camera viewpoints may change, but some researchers noted that fixed-camera setups (common in practice) reduce the relative advantage. The work was frequently cited alongside Act3D and RVT-2 in discussions of 3D manipulation.

OPEN QUESTIONS:
Can DP3 be extended with language conditioning to create a 3D-aware VLA? How should point cloud features be combined with pretrained 2D features for the best of both worlds? What is the minimum point cloud quality needed for reliable DP3 policies? Can DP3-style approaches scale to whole-body manipulation or mobile manipulation with large-scale 3D observations?""",

    27: """STRENGTHS:
Act3D introduces 3D feature field representations to multi-task robotic manipulation, using Transformers to process 3D voxel features and predict actions in 3D space. The coarse-to-fine 3D feature field allows the model to efficiently represent large workspaces while maintaining fine-grained precision where it matters. The approach naturally handles the 3D structure of manipulation, enabling better generalization across viewpoints and object configurations. Multi-task training with a single architecture shows that 3D representations can serve as a shared backbone for diverse manipulation skills. The 3D action prediction (predicting where and how to grasp in 3D) is more interpretable and geometrically meaningful than flat action vector predictions.

LIMITATIONS:
The 3D voxelization introduces a resolution-precision trade-off: finer voxels provide better precision but increase memory and compute costs quadratically (cubically in 3D). The coarse-to-fine strategy mitigates this but adds architectural complexity. Act3D requires calibrated multi-view cameras or depth sensors to construct the 3D representation, which complicates the hardware setup. Training is memory-intensive due to 3D attention operations. The approach predicts discrete 3D positions (which voxel to act at) which may limit precision for tasks requiring sub-voxel accuracy. Language conditioning and semantic understanding are limited compared to VLA approaches that leverage pretrained VLMs. The multi-task results, while positive, are demonstrated on a limited set of tasks.

COMMUNITY RECEPTION:
Act3D was recognized as an important step toward geometrically grounded manipulation policies. The coarse-to-fine 3D feature field was seen as an elegant solution to the resolution-scalability trade-off. The paper was favorably compared with PerAct and C2F-ARM as part of the 3D manipulation policy lineage. Researchers appreciated the geometric interpretability of the approach. However, the lack of language grounding and the hardware requirements limited adoption compared to simpler 2D VLA approaches. Act3D influenced the design of subsequent 3D manipulation methods and was frequently cited in discussions of the role of 3D representations in robot learning.

OPEN QUESTIONS:
Can Act3D's 3D feature fields be integrated with VLM backbones for language-conditioned 3D manipulation? How should the coarse-to-fine resolution be adapted dynamically based on task requirements? Can the approach be extended to deformable objects where the 3D structure changes during manipulation? How does Act3D compare with implicit 3D representations (NeRF-based) for manipulation planning?""",

    28: """STRENGTHS:
RVT-2 achieves impressive sample efficiency, learning precise manipulation from very few demonstrations by leveraging multi-view attention and re-rendering of the input scene from canonical viewpoints. The virtual re-rendering approach is clever: rather than processing raw camera views, RVT-2 renders the scene from a set of learned virtual viewpoints optimized for action prediction. This provides a form of data augmentation and viewpoint normalization that dramatically improves generalization. The model achieves state-of-the-art results on RLBench with 5-10x fewer demonstrations than competing methods. The approach scales well to multi-task settings with a single model handling diverse manipulation tasks. Inference is fast compared to methods that require iterative 3D feature field construction.

LIMITATIONS:
The virtual re-rendering approach requires accurate 3D scene reconstruction, which depends on reliable depth information and camera calibration. The method's performance is sensitive to the quality of the 3D reconstruction -- noisy depth or miscalibrated cameras degrade the re-rendered views and downstream predictions. The learned virtual viewpoints may not generalize well across very different workspace configurations. The evaluation is primarily on RLBench simulation, and real-world results are more limited. The approach predicts actions as discrete locations in the re-rendered views, inheriting the resolution limitations of the rendering. Language conditioning is limited to task identification rather than fine-grained instruction following. The method does not explicitly handle occlusions in the re-rendered views.

COMMUNITY RECEPTION:
RVT-2 was very well-received for its strong sample efficiency results and practical design. The virtual re-rendering idea was seen as a creative and effective solution to the viewpoint generalization problem. The paper built on and clearly improved over the original RVT, and the community appreciated the consistent improvement. RVT-2 became a standard baseline on RLBench and influenced subsequent work on efficient 3D manipulation. Some researchers noted that the RLBench-centric evaluation makes it hard to assess real-world applicability. The method was compared favorably with Act3D and DP3, with each approach having different strengths (RVT-2 for sample efficiency, DP3 for 3D generalization, Act3D for resolution).

OPEN QUESTIONS:
How well does the virtual re-rendering approach transfer to real-world scenarios with imperfect depth and calibration? Can the learned virtual viewpoints be interpreted or constrained to improve reliability? How should RVT-2's efficient 3D representation be combined with VLM-scale language understanding? Can the approach be extended to handle dynamic scenes where the 3D structure changes during manipulation? What is the fundamental limit of sample efficiency for manipulation learning?""",
}


def seed():
    admin = get_admin_client()
    for number, review in REVIEWS.items():
        admin.table("papers").update({"peer_reviews": review}).eq("number", number).execute()
        print(f"  Seeded peer review for paper #{number}")


if __name__ == "__main__":
    seed()
    print("Done!")
